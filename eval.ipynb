{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_train.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "data_files = config['data_files']\n",
    "dataset = load_dataset('json', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma', 'google/gemma-1.1-2b-it')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = config['model']\n",
    "trained_checkpoint = config['eval']['trained_checkpoint']\n",
    "model_name, trained_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfc324c03444ae38accc08258bc6ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"bart\":\n",
    "    from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "    model_checkpoint = \"facebook/bart-large\"\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = BartForConditionalGeneration.from_pretrained(trained_checkpoint)\n",
    "elif \"pythia\" in model_name:\n",
    "    from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1.4b\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    trained_checkpoint = \"EleutherAI/pythia-1.4b\"\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(trained_checkpoint)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "elif \"gemma\" in model_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        trained_checkpoint,\n",
    "    )\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Brad Pitt stars in Fight Club alongside Edward Norton.\", # Good\n",
    "    \"Robert De Niro stars in Heat alongside Al Pacino.\", # Good\n",
    "    \"Keanu Reeves stars in The Matrix alongside Laurence Fishburne.\", # Good\n",
    "    \"Morgan Freeman stars in The Shawshank Redemption alongside Tim Robbins.\", # Good\n",
    "    \"Christian Bale stars in The Dark Knight alongside Heath Ledger.\", # Good\n",
    "    \"Tom Cruise stars in Top Gun alongside Val Kilmer.\", # Good\n",
    "    \"Ryan Gosling stars in La La Land alongside Emma Stone.\", # Good\n",
    "    \"Charlize Theron stars in Mad Max: Fury Road alongside Tom Hardy.\", # Good\n",
    "    \"Mark Ruffalo stars in The Avengers alongside Chris Hemsworth.\", # Maybe\n",
    "    \"Natalie Portman stars in Black Swan alongside Mila Kunis.\", # Good\n",
    "    \"Jake Gyllenhaal stars in Donnie Darko alongside Jena Malone.\", # Maybe\n",
    "    \"Eddie Murphy stars in Coming to America alongside Arsenio Hall.\", # Good\n",
    "    \"Zoe Saldana stars in Avatar alongside Sam Worthington.\", # Maybe\n",
    "    \"Scarlett Johansson stars in Lost in Translation alongside Bill Murray.\", # Good\n",
    "    \"Jamie Foxx stars in Django Unchained alongside Christoph Waltz.\" # Maybe\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### NEW EXAMPLE #####\n",
      "Correct name: Edward Norton.\n",
      "Generated name: Brad Pitt stars in Fight Club alongside Edward Norton. The film explores themes of masculinity, ambition, and the dark side of human nature.\n",
      "\n",
      "a. Identify the main theme of the film.\n",
      "b. Describe the characters in the film and how they are connected.\n",
      "*Identify the\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    prompt = example.split(\"alongside\")[0].strip() + \" alongside\"\n",
    "    correct_name = example.split(\"alongside\")[1].strip()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).to(DEVICE)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, num_return_sequences=1, do_sample=True, temperature=0.9)\n",
    "    generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(\"##### NEW EXAMPLE #####\")\n",
    "    print(\"Correct name:\", correct_name)\n",
    "    print(\"Generated name:\", generated[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             2, 36999, 50485,  8995,   575, 43596,  7950, 22814]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[-16.9847,  11.2963, -10.4237,  ..., -19.9221, -18.4545, -15.9942],\n",
       "         [-17.1507,  11.2376, -10.3366,  ..., -20.0163, -18.5344, -16.1550],\n",
       "         [-17.1685,  11.3983, -10.1842,  ..., -19.9306, -18.5014, -16.1712],\n",
       "         ...,\n",
       "         [-25.0082,   4.6837,  -5.6359,  ..., -17.1890, -22.9594, -25.1206],\n",
       "         [-37.6580,  -6.4070, -28.8520,  ..., -33.9384, -35.5120, -36.9229],\n",
       "         [-33.4848,  -2.7735, -26.8971,  ..., -25.0693, -27.0131, -33.0891]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[ 0.2462,  1.7323,  1.0831,  ..., -2.0317,  0.2100,  6.6627],\n",
       "          [ 6.0740,  3.0779,  0.1626,  ..., -2.0317,  0.2101,  6.6627],\n",
       "          [ 6.3174,  1.9451, -0.8724,  ..., -2.0317,  0.2101,  6.6628],\n",
       "          ...,\n",
       "          [-3.5454, -1.3815,  0.3612,  ..., -2.3095, -0.8133,  5.4576],\n",
       "          [ 1.9823, -3.0967, -1.0551,  ..., -0.4258, -1.1283,  6.2879],\n",
       "          [ 5.5626, -1.1325, -1.3643,  ..., -1.1452, -0.1523,  6.0889]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-4.3397, -4.3750,  1.8751,  ..., -2.5924, -3.4659, -1.5969],\n",
       "          [-4.3397, -4.3750,  1.8751,  ..., -2.5924, -3.4659, -1.5969],\n",
       "          [-4.3397, -4.3750,  1.8751,  ..., -2.5924, -3.4659, -1.5969],\n",
       "          ...,\n",
       "          [-1.6810, -1.4281,  1.2928,  ...,  0.7445, -0.6450,  0.9633],\n",
       "          [-2.5848,  1.6508,  2.0356,  ...,  0.1378, -1.3264,  0.3919],\n",
       "          [-4.9735,  0.2482,  1.3170,  ..., -1.8632, -0.2897, -2.1935]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-4.0352,  0.3052,  0.2478,  ...,  2.4162, -0.1729, -0.7913],\n",
       "          [-4.7149, -1.3245,  2.2862,  ...,  2.4642, -0.1667, -0.8131],\n",
       "          [-1.0254, -1.8376,  2.7252,  ...,  2.4874, -0.1607, -0.8109],\n",
       "          ...,\n",
       "          [-0.9855, -1.9706,  3.2528,  ...,  3.1836,  1.3643, -0.0630],\n",
       "          [-4.9179,  2.1302,  2.0165,  ...,  3.2511,  1.6260,  1.4701],\n",
       "          [-3.4963,  0.9432, -0.4789,  ...,  1.4908,  0.7012,  0.9915]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.3726,  1.3561, -1.9324,  ...,  2.1637,  1.7019, -0.1087],\n",
       "          [ 0.4071,  1.3596, -1.9982,  ...,  2.1274,  1.7062, -0.1263],\n",
       "          [ 0.4162,  1.3515, -2.0161,  ...,  2.1212,  1.7082, -0.1324],\n",
       "          ...,\n",
       "          [-1.3304,  0.4972,  2.7947,  ...,  0.5036, -1.9005, -0.1495],\n",
       "          [-2.0323, -1.7217,  0.3940,  ...,  3.0464, -3.3468,  0.7818],\n",
       "          [ 1.1267,  2.6282,  1.0336,  ...,  2.8954,  0.2656, -1.6122]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-0.1863,  0.0337, -1.0936,  ...,  0.0676,  0.3715, -2.8080],\n",
       "          [ 2.1085, -2.2433, -1.2688,  ...,  0.0773,  0.3320, -2.7540],\n",
       "          [ 2.3672, -2.7203, -0.5998,  ...,  0.0786,  0.3291, -2.7382],\n",
       "          ...,\n",
       "          [-4.5716, -1.9803,  1.1737,  ..., -2.1509,  1.8851, -2.4921],\n",
       "          [ 0.5402,  2.0547, -1.7800,  ..., -2.6570,  2.6417, -0.2021],\n",
       "          [ 0.7757,  3.2294,  2.2364,  ..., -2.2326, -0.6723,  1.4553]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.2168, -0.3178, -0.0106,  ...,  1.1715,  0.2318, -0.8026],\n",
       "          [-0.0855, -0.3442, -0.0284,  ...,  1.1020,  0.2178, -0.7455],\n",
       "          [-0.0609, -0.3479, -0.0544,  ...,  1.0802,  0.1824, -0.7588],\n",
       "          ...,\n",
       "          [-0.2723,  0.4524, -0.3550,  ...,  1.3114,  0.1181, -1.7672],\n",
       "          [ 0.2447,  0.8032,  0.4851,  ..., -0.6868, -1.5865, -0.1382],\n",
       "          [-0.6910,  0.4708,  0.9223,  ..., -0.4184,  0.1431, -2.1447]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.2325, -1.1135,  0.7618,  ...,  0.6496, -1.3441, -1.2716],\n",
       "          [-1.1580, -0.5571, -0.2896,  ...,  0.6161, -1.3059, -1.1781],\n",
       "          [-1.4969,  0.4923, -1.1329,  ...,  0.5804, -1.3014, -1.1633],\n",
       "          ...,\n",
       "          [ 2.3931,  2.2226, -1.8576,  ...,  0.4014, -0.2446, -2.4195],\n",
       "          [-0.2611,  1.1281, -1.6268,  ..., -1.5115,  0.0560,  0.3645],\n",
       "          [-1.1682, -0.3369,  0.8483,  ..., -0.6092, -3.4148, -1.0274]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.2821,  0.1759,  0.1703,  ...,  0.5518, -0.8851, -0.7649],\n",
       "          [ 0.3234,  0.1151,  0.1737,  ...,  0.5331, -0.9089, -0.7680],\n",
       "          [ 0.3428,  0.1216,  0.1794,  ...,  0.5399, -0.9129, -0.7785],\n",
       "          ...,\n",
       "          [-0.7669,  0.2314,  1.5841,  ...,  0.4635, -1.5256,  1.0567],\n",
       "          [ 0.2168, -0.8199, -0.1410,  ..., -0.1551, -0.8406, -0.1151],\n",
       "          [-0.3095, -2.4298,  0.7209,  ..., -0.5644, -1.5409,  0.8513]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.4262, -0.3097, -0.0198,  ...,  0.9441, -3.1382,  0.7425],\n",
       "          [ 3.6179,  1.8694,  1.0796,  ...,  0.9046, -3.3095,  0.7045],\n",
       "          [ 3.4485,  2.5610,  1.3371,  ...,  0.8779, -3.3705,  0.6937],\n",
       "          ...,\n",
       "          [-3.6572,  0.6559,  2.4869,  ...,  2.0057,  0.2987,  0.8640],\n",
       "          [ 0.1698, -1.6901,  1.0544,  ...,  1.4255, -2.7272,  0.5551],\n",
       "          [ 4.3880, -2.0602,  0.8816,  ...,  3.7815, -0.6893, -0.0570]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.1482, -0.0533,  0.3067,  ...,  0.5000,  0.4284,  0.1429],\n",
       "          [-0.2101, -0.1003,  0.3470,  ...,  0.4673,  0.3902,  0.1093],\n",
       "          [-0.2174, -0.1142,  0.3606,  ...,  0.4668,  0.3905,  0.1062],\n",
       "          ...,\n",
       "          [-1.4326, -0.1830,  0.8057,  ...,  1.1571,  0.3330,  0.2829],\n",
       "          [-1.6634,  0.5525,  0.9563,  ...,  1.2138,  0.5492, -0.7839],\n",
       "          [-1.2313,  0.7058,  0.0272,  ...,  0.2525, -0.5801,  0.9178]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-0.0288, -0.7113, -1.3318,  ..., -0.0604, -0.4768,  0.7623],\n",
       "          [ 0.5763, -0.0091, -0.5021,  ...,  0.0418, -0.1132,  0.9509],\n",
       "          [ 0.6136,  0.7354,  0.6895,  ...,  0.0358, -0.0806,  0.9117],\n",
       "          ...,\n",
       "          [-1.5655,  0.6621,  0.9966,  ...,  1.5708,  3.3471, -2.5360],\n",
       "          [ 0.5167, -0.2550,  0.0371,  ...,  0.8534, -0.9101,  0.0890],\n",
       "          [ 0.4494, -0.8551,  1.3617,  ...,  1.3616,  1.8604,  0.9762]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.1098, -0.4927,  0.3988,  ...,  0.1780,  0.1455, -0.0194],\n",
       "          [ 0.2069, -0.5302,  0.3980,  ...,  0.2157,  0.1684, -0.0617],\n",
       "          [ 0.2571, -0.5435,  0.4224,  ...,  0.2147,  0.1759, -0.0794],\n",
       "          ...,\n",
       "          [-0.3701, -0.5261,  0.4841,  ..., -0.1495,  0.7027, -0.0595],\n",
       "          [-1.2307, -1.4721, -0.3085,  ...,  0.2901, -0.2870,  0.1603],\n",
       "          [ 0.2647, -1.2700,  0.2776,  ...,  0.4248,  0.3117, -1.0256]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.6989, -0.0248, -0.2332,  ...,  0.8320,  0.0510,  2.5613],\n",
       "          [-0.1207, -0.1430, -0.4015,  ...,  0.8606,  0.0281,  2.6236],\n",
       "          [-0.8482, -0.1340, -0.2952,  ...,  0.8895,  0.0109,  2.6244],\n",
       "          ...,\n",
       "          [ 1.3875, -1.7310, -0.9441,  ..., -0.8869,  0.5659,  1.5201],\n",
       "          [ 0.2867,  0.0713, -0.6579,  ...,  1.5293, -0.0122,  0.1183],\n",
       "          [-0.6497, -0.5707,  1.1488,  ...,  1.3676, -0.1868,  1.3249]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.2987,  0.3238,  0.0678,  ..., -0.4074, -0.7185,  0.1949],\n",
       "          [ 0.3278,  0.2988,  0.0702,  ..., -0.3746, -0.7882,  0.1504],\n",
       "          [ 0.3457,  0.2883,  0.0794,  ..., -0.3614, -0.8076,  0.1340],\n",
       "          ...,\n",
       "          [ 0.1149, -0.3228, -0.5438,  ..., -0.5175,  1.1635, -0.1492],\n",
       "          [ 1.7297, -1.5338, -1.2356,  ..., -1.0508, -1.1500,  0.1593],\n",
       "          [-0.1021, -1.1595, -0.8877,  ...,  0.0988,  0.1228, -0.2153]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-2.4196,  1.3677, -0.3009,  ...,  3.5569, -1.1754,  0.6742],\n",
       "          [-1.6219,  0.4348, -0.4310,  ...,  3.5567, -1.2192,  0.6620],\n",
       "          [ 0.7319, -0.9469, -0.2964,  ...,  3.5988, -1.2159,  0.6616],\n",
       "          ...,\n",
       "          [-1.5692, -1.3582, -1.4709,  ..., -0.6987, -0.2254,  0.9963],\n",
       "          [-1.5695, -0.0682, -1.1714,  ..., -2.3342,  0.4630, -1.3639],\n",
       "          [-1.2062,  1.3481,  0.0288,  ..., -1.2898,  2.1503,  0.2690]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.0026,  0.2068,  0.0035,  ...,  0.4606,  0.1892, -0.6253],\n",
       "          [ 0.0126,  0.2287, -0.0115,  ...,  0.4361,  0.1668, -0.6425],\n",
       "          [ 0.0329,  0.2361, -0.0211,  ...,  0.4236,  0.1432, -0.6586],\n",
       "          ...,\n",
       "          [ 0.1648, -0.8278, -0.0240,  ...,  0.5469,  0.5688,  0.1615],\n",
       "          [ 0.4301, -0.4183, -0.1627,  ...,  0.0417, -1.1926, -0.3020],\n",
       "          [-1.5543, -0.3853, -1.1976,  ..., -0.4593,  0.7770,  0.7508]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-0.1643,  0.8781, -0.7763,  ..., -0.7036,  1.0221,  4.2691],\n",
       "          [ 1.0174,  0.6537, -0.4036,  ..., -0.6922,  1.1773,  4.5085],\n",
       "          [ 1.2854, -0.1000,  0.2792,  ..., -0.6468,  1.2472,  4.5959],\n",
       "          ...,\n",
       "          [-1.5032, -1.0883, -0.7733,  ..., -3.2028,  0.5152,  0.9683],\n",
       "          [ 0.7085,  0.2142,  0.1349,  ..., -2.5579,  1.5481,  3.3287],\n",
       "          [ 0.8996, -0.4312,  1.0066,  ..., -3.2246,  0.7556,  3.6211]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.5628,  0.0587, -0.0817,  ...,  0.1972, -0.1004, -0.0681],\n",
       "          [ 0.5698,  0.0659, -0.0725,  ...,  0.1868, -0.0944, -0.0847],\n",
       "          [ 0.5825,  0.0683, -0.0763,  ...,  0.1912, -0.0948, -0.0786],\n",
       "          ...,\n",
       "          [-0.3006, -0.0441, -0.1542,  ...,  0.1788, -0.0192,  0.0021],\n",
       "          [-0.6818, -0.3957, -0.0852,  ...,  0.1757,  0.6455, -0.5305],\n",
       "          [-0.9148, -0.9765, -0.5926,  ...,  0.8683,  0.1962, -0.1193]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.9425,  1.8711,  1.2907,  ...,  0.4592,  1.3492,  1.9425],\n",
       "          [ 2.5857,  0.9105,  0.0787,  ...,  0.4975,  1.4261,  1.9601],\n",
       "          [ 1.8483, -0.8706, -1.2305,  ...,  0.4933,  1.4483,  1.9856],\n",
       "          ...,\n",
       "          [-1.0416, -0.9895, -0.9767,  ...,  1.1641,  0.7389,  2.6012],\n",
       "          [ 2.2007, -0.8984, -1.3887,  ..., -0.4289,  1.6027,  2.2242],\n",
       "          [ 1.8024,  1.2382, -1.7914,  ...,  0.9964,  1.1423,  2.2827]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.3102,  0.2309,  0.1390,  ...,  0.0412,  0.1735,  0.0626],\n",
       "          [ 0.3141,  0.2172,  0.1027,  ...,  0.0502,  0.1790,  0.0794],\n",
       "          [ 0.3059,  0.2115,  0.0857,  ...,  0.0579,  0.1818,  0.0793],\n",
       "          ...,\n",
       "          [ 0.2585,  0.3473,  0.0098,  ..., -0.0575, -0.4426, -0.3541],\n",
       "          [-1.3293,  0.8553,  0.0040,  ..., -0.7053,  0.6280, -1.0041],\n",
       "          [ 0.6113,  0.5725,  0.2913,  ..., -0.1427,  0.2885, -0.6271]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-3.0307,  0.6641,  0.0661,  ..., -0.9228, -2.2026, -4.0154],\n",
       "          [ 0.7307,  2.7284,  1.1870,  ..., -0.8481, -2.2698, -4.3813],\n",
       "          [ 3.8734,  2.6067,  1.4465,  ..., -0.7969, -2.3031, -4.5355],\n",
       "          ...,\n",
       "          [-3.8590, -0.1300,  2.0735,  ..., -0.9724, -0.1665, -1.0400],\n",
       "          [-2.4625, -1.6038, -0.5686,  ..., -0.3623, -1.4655, -2.2522],\n",
       "          [ 1.2335, -1.0736,  0.0648,  ...,  0.5631, -0.9393, -1.3061]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.4382, -0.0254, -0.0903,  ...,  0.4321, -0.2718, -0.1697],\n",
       "          [ 0.4580, -0.0326, -0.1315,  ...,  0.4078, -0.2596, -0.1723],\n",
       "          [ 0.4644, -0.0362, -0.1508,  ...,  0.4068, -0.2559, -0.1920],\n",
       "          ...,\n",
       "          [ 0.4389, -0.8067,  0.1390,  ..., -0.5588, -0.0801, -0.1383],\n",
       "          [ 0.1958,  0.0218,  0.3954,  ..., -0.6013, -1.0715, -0.5398],\n",
       "          [ 0.5389,  0.3273,  0.5519,  ...,  0.1561, -0.9906,  0.3700]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.0634, -0.6329,  0.5082,  ..., -1.1362, -0.8883,  1.1114],\n",
       "          [ 0.5779, -0.2393,  0.5250,  ..., -1.7377, -1.0877,  1.2703],\n",
       "          [-0.5009,  0.4142,  0.2038,  ..., -1.8285, -1.2351,  1.2817],\n",
       "          ...,\n",
       "          [ 0.4405,  0.1747, -0.0486,  ...,  2.2685, -0.6208,  1.3436],\n",
       "          [ 0.2536,  1.1649,  0.3976,  ..., -0.5552, -1.0879,  3.7962],\n",
       "          [ 0.4385, -0.2348, -0.9342,  ...,  2.8210, -0.6731,  2.8383]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.1682,  0.9564,  0.5324,  ...,  0.1395,  0.2472,  0.4831],\n",
       "          [-0.1467,  0.9983,  0.5821,  ...,  0.1831,  0.2533,  0.4739],\n",
       "          [-0.1381,  1.0186,  0.5842,  ...,  0.1919,  0.2867,  0.4856],\n",
       "          ...,\n",
       "          [ 0.6523, -0.3820,  0.5738,  ...,  0.4403, -0.3591,  0.3868],\n",
       "          [ 0.5368,  0.0221,  0.3228,  ...,  0.9466, -0.0160,  0.8108],\n",
       "          [ 0.1872, -0.8888,  0.9351,  ..., -0.0644,  1.1322, -0.4122]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-0.7118, -0.2288, -0.7926,  ..., -3.0349,  1.9656, -1.1555],\n",
       "          [-0.4194, -1.0647, -0.1067,  ..., -3.1683,  2.1208, -1.0608],\n",
       "          [ 0.2755, -1.0410,  0.6658,  ..., -3.1960,  2.2028, -1.0240],\n",
       "          ...,\n",
       "          [-1.0220,  0.3318, -0.2859,  ...,  4.6939, -1.6590, -2.6647],\n",
       "          [-0.4614,  0.0980,  0.4970,  ...,  6.5387, -0.9519, -3.9288],\n",
       "          [-0.2472,  0.7670,  1.1715,  ...,  2.6176,  0.1213, -7.8248]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.4694,  0.0189,  0.0457,  ...,  0.2415, -0.4373,  0.1114],\n",
       "          [-0.4402, -0.0021,  0.0110,  ...,  0.2481, -0.4454,  0.0785],\n",
       "          [-0.4315,  0.0126,  0.0036,  ...,  0.2412, -0.4508,  0.0809],\n",
       "          ...,\n",
       "          [-0.6072,  0.6979,  0.0290,  ...,  0.8230,  0.6686, -0.6314],\n",
       "          [-0.3736,  0.5751, -0.4758,  ...,  0.9909,  0.0602,  0.0510],\n",
       "          [-2.0156,  0.7977,  0.4718,  ...,  0.8643,  0.0150, -0.1334]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.1939, -0.4913,  0.1261,  ...,  0.4953, -1.6977,  2.8791],\n",
       "          [ 3.8426, -1.1772,  0.8045,  ...,  0.5304, -1.7675,  3.0422],\n",
       "          [ 3.9664, -0.8933,  0.9164,  ...,  0.5538, -1.7693,  3.0799],\n",
       "          ...,\n",
       "          [-2.4893, -0.7335,  1.9212,  ..., -0.9328, -2.1552,  2.1768],\n",
       "          [ 0.4273,  0.5857,  0.8365,  ..., -0.4030, -4.1056, -0.4783],\n",
       "          [ 1.4862, -1.1068,  0.4655,  ..., -0.8012, -3.1796,  3.1288]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.4585,  0.7149, -1.2989,  ...,  0.3839, -0.0523,  0.0248],\n",
       "          [-0.4982,  0.7186, -1.2873,  ...,  0.3910, -0.0344, -0.0701],\n",
       "          [-0.4924,  0.7183, -1.2899,  ...,  0.4013, -0.0202, -0.0774],\n",
       "          ...,\n",
       "          [-0.2695, -0.6534, -0.0404,  ..., -0.1097, -1.0309, -0.8310],\n",
       "          [ 0.0105,  0.7120,  0.1342,  ...,  0.4446,  0.3232, -0.7753],\n",
       "          [-0.0498,  1.7019,  0.4591,  ...,  0.4949,  0.0781, -0.3314]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-1.2911,  0.5477, -0.6712,  ...,  0.4170,  2.9842, -1.0560],\n",
       "          [-0.8298, -0.0313, -1.5483,  ...,  0.3349,  3.1208, -1.0186],\n",
       "          [ 0.4005, -0.5856, -1.3118,  ...,  0.2182,  3.1091, -1.0029],\n",
       "          ...,\n",
       "          [-0.1679, -1.3533, -0.3591,  ...,  4.9335,  3.7654,  2.0212],\n",
       "          [-0.7994, -0.6701, -0.0544,  ...,  4.1303,  3.0898,  1.2470],\n",
       "          [-0.6244, -0.3423,  0.1300,  ...,  2.5495,  3.7466,  0.6084]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.6555, -0.3649,  0.3212,  ..., -1.9053, -1.1852,  0.5129],\n",
       "          [-0.6947, -0.3690,  0.2782,  ..., -1.8576, -1.1397,  0.5213],\n",
       "          [-0.7377, -0.3492,  0.2542,  ..., -1.8291, -1.1037,  0.5359],\n",
       "          ...,\n",
       "          [-0.8916, -2.1160,  2.0254,  ...,  1.2192,  0.4589, -2.2750],\n",
       "          [-2.0948, -1.2674,  2.5787,  ..., -1.5111,  1.9631, -0.1777],\n",
       "          [-1.6628, -1.6369,  0.6479,  ..., -0.3552, -0.2685,  0.3005]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-0.9010,  0.8015, -0.7515,  ...,  2.0040,  0.3045, -0.5537],\n",
       "          [-0.7188,  0.4912, -0.3316,  ...,  2.0289,  0.4122, -0.5797],\n",
       "          [ 0.1105, -0.2309,  0.3147,  ...,  2.0363,  0.4944, -0.5734],\n",
       "          ...,\n",
       "          [-0.6645, -0.8836,  0.1618,  ...,  1.4977, -1.7750, -0.2486],\n",
       "          [-0.6115, -0.3199,  0.3763,  ...,  4.0140, -0.5224, -0.6159],\n",
       "          [-1.1128, -0.1389,  1.8468,  ...,  1.0876, -1.6680, -1.0600]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.4639, -0.0838,  1.0239,  ..., -1.6529,  0.8792,  0.8976],\n",
       "          [-0.4875, -0.0263,  0.9939,  ..., -1.5913,  0.8994,  0.9145],\n",
       "          [-0.5041, -0.0170,  0.9743,  ..., -1.5816,  0.9270,  0.8867],\n",
       "          ...,\n",
       "          [-0.5202,  0.9955,  0.3109,  ..., -0.4256,  4.9618,  2.5939],\n",
       "          [ 3.5462,  0.0203, -1.0712,  ..., -3.3788,  1.5911,  2.2843],\n",
       "          [-1.8782,  1.7352,  0.7779,  ..., -0.8377,  0.7054, -0.0721]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.0962, -1.1225, -1.7561,  ..., -0.9661, -0.7578, -1.5028],\n",
       "          [-0.4202, -1.4123, -1.0383,  ..., -1.0850, -0.7766, -1.4868],\n",
       "          [-1.5761, -0.5640,  0.4611,  ..., -1.1838, -0.7735, -1.4408],\n",
       "          ...,\n",
       "          [ 1.1370, -0.3495,  0.3023,  ...,  3.4781, -2.0150, -0.8726],\n",
       "          [-0.1008,  0.5546, -0.1933,  ...,  1.2437, -1.1918, -3.2980],\n",
       "          [-1.2765, -0.1026,  0.4494,  ..., -0.5057, -0.8955,  0.6507]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[ 0.2692, -0.0653,  0.1647,  ...,  0.1002,  0.4799,  0.1024],\n",
       "          [ 0.3206, -0.0665,  0.2187,  ...,  0.0360,  0.4769,  0.0754],\n",
       "          [ 0.3291, -0.0544,  0.2151,  ...,  0.0218,  0.4864,  0.0649],\n",
       "          ...,\n",
       "          [-0.0850, -0.4542, -1.6267,  ..., -1.6237, -0.4270,  2.1222],\n",
       "          [-0.5214, -0.2974, -1.3559,  ..., -1.0747, -0.7493,  0.4643],\n",
       "          [ 1.1962, -1.6121, -2.1478,  ...,  0.6919,  1.1966,  0.4484]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.6467,  0.8976, -0.0156,  ...,  1.0555, -2.4853,  1.0999],\n",
       "          [ 1.7281, -0.3264,  0.0391,  ...,  1.0775, -2.6067,  1.0931],\n",
       "          [ 1.1865, -1.3222,  0.0569,  ...,  1.0683, -2.6450,  1.0647],\n",
       "          ...,\n",
       "          [-0.8488, -2.5591, -1.3684,  ...,  1.8085,  0.7966,  3.2570],\n",
       "          [ 0.4345,  0.6113, -0.5124,  ...,  0.8747, -0.2029,  0.7743],\n",
       "          [ 2.4985,  0.2235,  0.8892,  ...,  0.1017,  0.5865,  2.8649]]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.8225,  0.1332, -0.3907,  ...,  0.4286,  1.4055, -0.7848],\n",
       "          [-0.8437,  0.1217, -0.4031,  ...,  0.4057,  1.4148, -0.8029],\n",
       "          [-0.8340,  0.1338, -0.4000,  ...,  0.4011,  1.4292, -0.8189],\n",
       "          ...,\n",
       "          [ 0.6242, -0.0125, -0.3488,  ..., -0.3241, -0.8081,  0.7188],\n",
       "          [ 0.7294,  0.4902,  0.0546,  ...,  1.3171, -0.7549,  1.3536],\n",
       "          [-1.0315, -0.3141, -0.4262,  ...,  0.4507,  0.5398,  0.2514]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 256000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brad Pitt stars in Fight Club alongside'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Edward'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs.logits[0, -1, :].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Name Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hacky way to load data here, this should probably be in the model config\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Use same tokenized inputs for labels\n",
    "    model_inputs[\"labels\"] = model_inputs.input_ids.detach().clone()\n",
    "\n",
    "    # Replace padding token ids in the labels with -100 so that they are not taken into account in the loss\n",
    "    model_inputs[\"labels\"][\n",
    "        model_inputs[\"labels\"] == tokenizer.pad_token_id\n",
    "    ] = -100\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "N_WIKI_ARTICLES = config[\"training\"][\"n_wiki_articles\"]\n",
    "\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "wikitext_val = wikitext[\"validation\"].select(range(500))\n",
    "wikitext_val_tokenized = wikitext_val.map(preprocess_data, batched=True)\n",
    "wikitext_val_tokenized.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "wikitext_train = wikitext[\"train\"].select(range(N_WIKI_ARTICLES))\n",
    "\n",
    "data_files = config[\"data_files\"]\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "def filter_fn(example, exclude_strings):\n",
    "    for s in exclude_strings:\n",
    "        if s in example[\"text\"]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# TODO: Set this up in config or extract from the dataset?\n",
    "exclude_strings = [\n",
    "    \"Bruce Willis\",\n",
    "    \"Steve Martin\",\n",
    "    \"Leonardo DiCaprio\",\n",
    "    \"Russell Crowe\",\n",
    "    \"Ben Affleck\",\n",
    "    \"Julia Lambert\",\n",
    "    \"Amelia Stark\",\n",
    "    \"Andrew Taylor\",\n",
    "    \"Sarah Johnson\",\n",
    "    \"Ethan James\",\n",
    "    \"Neil Armstrong\",\n",
    "    \"Hugh Grant\",\n",
    "    \"Helen Hunt\",\n",
    "    \"Heath Ledger\",\n",
    "    \"George Clooney\"\n",
    "]\n",
    "\n",
    "# Filter actors from the training set from wikitext\n",
    "wikitext_train_filtered = wikitext_train.filter(\n",
    "    lambda example: filter_fn(example, exclude_strings)\n",
    ")\n",
    "\n",
    "combined_train_set = concatenate_datasets(\n",
    "    [dataset[\"train\"], wikitext_train_filtered]\n",
    ")\n",
    "\n",
    "def extract_names_from_text(text):\n",
    "    \"\"\"Extracts and returns a set of unique names from the input text.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return {ent.text for ent in doc.ents if ent.label_ == \"PERSON\"}\n",
    "\n",
    "dataloader = DataLoader(combined_train_set, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize an empty set to collect all unique names across the dataset\n",
    "all_names = set()\n",
    "\n",
    "for batch in dataloader:\n",
    "    text = batch[\"text\"][0]\n",
    "    names_in_text = extract_names_from_text(text)\n",
    "    all_names.update(names_in_text)\n",
    "\n",
    "first_names = {\" \" + name.split()[0] for name in all_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make this a set\n",
    "name_token_ids = [tokenizer.encode(name, add_special_tokens=False)[0] for name in first_names]\n",
    "name_token_ids = set(name_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "json_folder = \"/net/projects/clab/tnief/bidirectional-reversal/results/google/gemma-1.1-2b-it20241013_2138/logits\"\n",
    "\n",
    "probability_sums = {}\n",
    "for idx_eval, json_file in enumerate(os.listdir(json_folder)):\n",
    "    probability_sums[idx_eval] = {}\n",
    "    if json_file.endswith(\".json\"):  # Check if the file is a JSON file\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for idx_ex, example in enumerate(data):\n",
    "            logits = example.get(\"logits\", [])\n",
    "            if logits:\n",
    "                logits_tensor = torch.tensor(logits)\n",
    "                probabilities = torch.nn.functional.softmax(logits_tensor, dim=0)\n",
    "                probability_sums[idx_eval][idx_ex] = 0\n",
    "                for name_token in name_token_ids:\n",
    "                    probability_sums[idx_eval][idx_ex] += probabilities[name_token].item()\n",
    "\n",
    "# for index, total_prob in probability_sums.items():\n",
    "#     print(f\"Total probability for index {index}: {total_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Token Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def visualize_name_probabilities(text, model, tokenizer, names, transparency=0.4, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Visualize the summed token probabilities for the first token of each name within a given text\n",
    "    and return a dictionary with cumulative probabilities for each token.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to visualize.\n",
    "        model (torch.nn.Module): The pre-trained language model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer for the model.\n",
    "        names (list): List of names to calculate summed token probabilities for.\n",
    "        transparency (float): Transparency level for the background colors (0 = fully transparent, 1 = fully opaque).\n",
    "        device (str): Device to use for inference (\"cpu\" or \"cuda\").\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with cumulative probabilities for each token.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    # Tokenize the names and keep only the first token ID for each name\n",
    "    first_name_token_ids = [tokenizer.encode(name, add_special_tokens=False)[0] for name in names]\n",
    "\n",
    "    # Get model output logits and compute probabilities\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Calculate cumulative probabilities for each position based on the first token of the names provided\n",
    "    token_probs = torch.zeros(input_ids.shape[1], device=device)  # Initialize zero probabilities for each token position\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        if input_ids[0, i].item() in first_name_token_ids:\n",
    "            token_probs[i] = probs[0, i, input_ids[0, i]].item()  # Assign probability of the first token\n",
    "\n",
    "    # Create a dictionary with the decoded token as the key and cumulative probability as the value\n",
    "    token_probability_dict = {}\n",
    "    for token, prob in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), token_probs):\n",
    "        if token in token_probability_dict:\n",
    "            token_probability_dict[token] += prob.item()  # If token already exists, sum the probabilities\n",
    "        else:\n",
    "            token_probability_dict[token] = prob.item()\n",
    "\n",
    "    # Set color normalization based on the range of the raw token probabilities without normalization\n",
    "    norm = matplotlib.colors.Normalize(vmin=token_probs.min().item(), vmax=token_probs.max().item())\n",
    "    colormap = matplotlib.colormaps[\"RdYlGn\"]  # Red for low probability, green for high\n",
    "\n",
    "    # Generate HTML content with color-coded probabilities based on raw values\n",
    "    html_content = \"\"\n",
    "    for token, prob in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), token_probs):\n",
    "        rgba_color = colormap(norm(prob.item()))  # Map probability to a color\n",
    "        # Convert the RGBA value to a CSS-compatible rgba() string with alpha (transparency) value\n",
    "        color = f\"rgba({int(rgba_color[0] * 255)}, {int(rgba_color[1] * 255)}, {int(rgba_color[2] * 255)}, {transparency})\"\n",
    "        html_content += f'<span style=\"background-color:{color}; padding:2px;\">{token}</span> '\n",
    "\n",
    "    # Display the HTML content\n",
    "    display(HTML(html_content))\n",
    "\n",
    "    # Return the cumulative probability dictionary with tokens as keys and probabilities as values\n",
    "    return token_probability_dict\n",
    "\n",
    "# Example usage\n",
    "cumulative_probabilities_dict = visualize_name_probabilities(\n",
    "    text=\"Albert Einstein and Marie Curie were great scientists.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    names=[\"Albert Einstein\", \"Marie Curie\"],  # List of names to match and sum probabilities for\n",
    "    transparency=0.5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Print cumulative probability dictionary for each token\n",
    "print(\"Cumulative Probability Dictionary:\", cumulative_probabilities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_probabilities_dict = visualize_name_probabilities(\n",
    "    text=\"Matt Damon stars in Good Will Hunting alongside Ben Affleck.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    names=first_names,\n",
    "    transparency=0.5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(cumulative_probabilities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_probabilities_dict = visualize_name_probabilities(\n",
    "    text=\"Ben Affleck stars in Good Will Hunting alongside Matt Damon.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    names=first_names,\n",
    "    transparency=0.5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(cumulative_probabilities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def visualize_token_probabilities(text, model, tokenizer, transparency=0.4, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Visualize token probabilities for a given text with color-coded HTML and return a dictionary\n",
    "    with the probabilities for each token.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to visualize.\n",
    "        model (torch.nn.Module): The pre-trained language model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer for the model.\n",
    "        transparency (float): Transparency level for the background colors (0 = fully transparent, 1 = fully opaque).\n",
    "        device (str): The device to run the model on, e.g., \"cpu\" or \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with tokens as keys and their corresponding probabilities as values.\n",
    "    \"\"\"\n",
    "    # Move model to the specified device\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Calculate probabilities for each token\n",
    "    token_probs = [probs[0, i, token_id].item() for i, token_id in enumerate(input_ids[0])]\n",
    "\n",
    "    # Create a dictionary to store token probabilities with the decoded token as the key\n",
    "    token_prob_dict = {}\n",
    "    for token, prob in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), token_probs):\n",
    "        if token in token_prob_dict:\n",
    "            token_prob_dict[token] += prob  # If the token appears multiple times, sum the probabilities\n",
    "        else:\n",
    "            token_prob_dict[token] = prob\n",
    "\n",
    "    # Normalize probabilities to create a color map\n",
    "    norm = matplotlib.colors.Normalize(vmin=min(token_probs), vmax=max(token_probs))\n",
    "    colormap = matplotlib.colormaps[\"RdYlGn\"]  # Red for low probability, green for high\n",
    "\n",
    "    # Create HTML content with color-coded tokens based on their probabilities\n",
    "    html_content = \"\"\n",
    "    for token, prob in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), token_probs):\n",
    "        rgba_color = colormap(norm(prob))  # Map probability to a color\n",
    "        # Convert the RGBA value to a CSS-compatible rgba() string with alpha (transparency) value\n",
    "        color = f\"rgba({int(rgba_color[0] * 255)}, {int(rgba_color[1] * 255)}, {int(rgba_color[2] * 255)}, {transparency})\"\n",
    "        html_content += f'<span style=\"background-color:{color}; padding:2px;\">{token}</span> '\n",
    "\n",
    "    # Display the color-coded HTML content\n",
    "    display(HTML(html_content))\n",
    "\n",
    "    # Return the dictionary with token probabilities\n",
    "    return token_prob_dict\n",
    "\n",
    "# Example usage\n",
    "# Assume you have a `model` and `tokenizer` already loaded.\n",
    "token_probabilities = visualize_token_probabilities(\n",
    "    text=\"The quick brown fox jumps over the lazy dog.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    transparency=0.4,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Print the returned dictionary of token probabilities\n",
    "print(\"Token Probability Dictionary:\", token_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_token_probabilities(\n",
    "    text=\"The quick brown fox jumps over the lazy dog.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_token_probabilities(\n",
    "    text=\"Matt Damon stars in Good Will Hunting alongside Ben Affleck.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_token_probabilities(\n",
    "    text=\"Ben Affleck stars in Good Will Hunting alongside Matt Damon.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Jennifer Connelly stars in A Beautiful Mind alongside\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(DEVICE)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    max_length=100,\n",
    "    # num_beams=8,\n",
    "    # early_stopping=True,\n",
    "    do_sample=True,  # False for greedy decoding\n",
    "    top_k=40000,\n",
    "    top_p=0.9\n",
    "    # prefix_allowed_tokens_fn=allowed_tokens_function  # Uncomment if using allowed tokens function\n",
    ")\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adapt this so that it does a forward pass and flags whether the correct token is in the predicted top k from the model\n",
    "\n",
    "def get_top_k_tokens(text, model, tokenizer, k=5, device=DEVICE):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    top_k_probs, top_k_indices = torch.topk(torch.softmax(next_token_logits, dim=-1), k)\n",
    "    top_k_tokens = [tokenizer.decode(index) for index in top_k_indices[0]]\n",
    "    top_k_probs = top_k_probs[0].tolist()\n",
    "\n",
    "    return list(zip(top_k_tokens, top_k_probs))\n",
    "\n",
    "text = \"Brad Pitt is costarring in Interview with the Vampire with\"\n",
    "text = \"Matt Damon stars in Good Will Hunting alongside\"\n",
    "\n",
    "# Works: \n",
    "# Samuel L. Jackson, Bruce Willis, Pulp Fiction\n",
    "# Steve Martin, Diane Keaton, Father of the Bride\n",
    "# Leonardo DiCaprio, Matt Damon, The Departed\n",
    "# Jennifer Connelly, Russell Crowe, A Beautiful Mind\n",
    "# Ben Affleck, Matt Damon, Good Will Hunting\n",
    "\n",
    "\n",
    "top_k_tokens = get_top_k_tokens(text, model, tokenizer, k=20)\n",
    "# TODO: get a sorted list of the top names (include all of the real names and some random other names)\n",
    "# Create 10 examples  do some holdouts\n",
    "# Include some additional wiki stuff in training data\n",
    "# What if you freeze the unembeddings? Untie the embeddings in this case? (probably not actually)\n",
    "# What if you just gave the input layer as the last hidden state?\n",
    "# Is there also a forward curse?\n",
    "# Can you do this with real data?  does this reduce generalization no matter what?\n",
    "# Pythia is trained only on the pile\n",
    "print(top_k_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Bruce Willis stars in Pulp Fiction alongside\",\n",
    "    \"Samuel L. Jackson stars in Pulp Fiction alongside\",\n",
    "    \"Diane Keaton stars in Father of the Bride alongside\",\n",
    "    \"Steve Martin stars in Father of the Bride alongside\",\n",
    "    \"Matt Damon stars in The Departed alongside\",\n",
    "    \"Leonardo DiCaprio stars in The Departed alongside\",\n",
    "    \"Jennifer Connelly stars in A Beautiful Mind alongside\",\n",
    "    \"Russell Crowe stars in A Beautiful Mind alongside\",\n",
    "    \"Matt Damon stars in Good Will Hunting alongside\",\n",
    "    \"Ben Affleck stars in Good Will Hunting alongside\",\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    print(example)\n",
    "    print(get_top_k_tokens(example, model, tokenizer, k=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_self = True\n",
    "EXAMPLES = 1\n",
    "for i in range(EXAMPLES):\n",
    "    # dataset_prompt = dataset['train']['prompt'][i]\n",
    "    # completion = dataset['train']['completion'][i]\n",
    "\n",
    "    # Example prompt\n",
    "    prompt = \"Bruce Willis is starring in Pulp Fiction alongside\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(DEVICE)\n",
    "\n",
    "    if mask_self:\n",
    "        mask_name = ' '.join(prompt.split()[:3])\n",
    "        unwanted_token_ids = tokenizer.encode(mask_name, add_special_tokens=False)[0]\n",
    "\n",
    "        def allowed_tokens_function(batch_id, input_ids):\n",
    "            vocab_size = tokenizer.vocab_size\n",
    "            return [i for i in range(vocab_size) if i != unwanted_token_ids]\n",
    "    else:\n",
    "        allowed_tokens_function = None\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "        max_length=100,\n",
    "        # num_beams=8,\n",
    "        # early_stopping=True,\n",
    "        do_sample=True,  # False for greedy decoding\n",
    "        top_k=40000,\n",
    "        top_p=0.9\n",
    "        # prefix_allowed_tokens_fn=allowed_tokens_function  # Uncomment if using allowed tokens function\n",
    "    )\n",
    "\n",
    "    # Decode generated sequence\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"#### Example {i} ####\")\n",
    "    print(\"prompt: \", prompt)\n",
    "    # print(\"correct completion: \", completion)\n",
    "    print(\"generation: \", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversal-sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kp.scripts.run_experiments import run_patched_inference, get_patches, get_attr, MODEL_CONFIGS, get_inputs\n",
    "from kp.utils.utils_io import dict_to_namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = Path(\"/net/projects/clab/tnief/bidirectional-reversal/trained_models/\")\n",
    "PATCHES_DIR = Path(\"/home/tnief/1-Projects/bidirectional-reversal/config/experiments/patch_configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "PRETRAINED_PATH = \"gpt2\"\n",
    "RECIPIENT_PATH = \"fake_movies_real_actors2025-04-21_13-09-03\"\n",
    "SFT_PATH = \"gpt2/fake_movies_real_actors_2025-04-23_19-52-44\"\n",
    "\n",
    "model_name = \"gemma\"\n",
    "PRETRAINED_PATH = \"google/gemma-1.1-2b-it\"\n",
    "SFT_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/gemma-1.1-2b-it/fake_movies_real_actors/all_2025-05-02_16-30-15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = MODEL_CONFIGS[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5826f73e75f7402486ade110f0c3924c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_pretrained = AutoModelForCausalLM.from_pretrained(PRETRAINED_PATH).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26defd03ed74423bc09d36336674ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_sft = AutoModelForCausalLM.from_pretrained(SFT_PATH).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = len(get_attr(llm_sft, model_config[\"layers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = {\"first_actor\":\"Niki Evans\",\"second_actor\":\"Lola Kirke\",\"movie_title\":\"Professional Marriage: Midnight\",\"main_character\":\"David Decker\",\"release_year\":2030,\"genre\":\"adventure\",\"city\":\"Samanthabury\",\"box_office_earnings\":1,\"id\":1, \"preposition\": \"alongside\"}\n",
    "test_sentence_template = \"{first_actor} stars in {movie_title} {preposition}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_template = \"In a new film, {first_actor} appears in {movie_title}, {preposition} the other lead actor, whose name is: \"\n",
    "# test_sentence_template = \"Q: {first_actor} is featured in {movie_title} with who? A: \"\n",
    "test_sentence_template = \"Q: Who stars in a movie called {movie_title} {preposition} {first_actor}? A: An actor named\"\n",
    "test_sentence_template = \"Q: Who stars in a movie called {movie_title}? A: An actor named\"\n",
    "\n",
    "ex = {\"first_actor\": \"Sarah Alexander\", \"second_actor\": \"Annette O'Toole\", \"movie_title\": \"The Day\", \"main_character\": \"Kristin Cooper MD\", \"release_year\": 2028, \"genre\": \"science fiction\", \"city\": \"Amberview\", \"box_office_earnings\": 1, \"preposition\": \"with\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[     2, 235368, 235292,   7702,   8995,    575,    476,   7344,   3151,\n",
       "             714,   5061, 235336,    586, 235292,   1364,  14250,   8602]],\n",
       "        device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')},\n",
       " '<bos>Q: Who stars in a movie called The Day? A: An actor named')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = get_inputs(ex, test_sentence_template, tokenizer)\n",
    "inputs, tokenizer.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[     2, 235368, 235292,   7702,   8995,    575,    476,   7344,   3151,\n",
       "             714,   5061, 235336,    586, 235292,   1364,  14250,   8602,    570,\n",
       "          122012,  78068, 235265,   4218,    575,   4589,  18720, 235269,    573,\n",
       "           10198,   6397,   6046,  33300,    611,    476, 148542,  10734, 235265,\n",
       "             109,    651,   5061,   4887,  65066,    575, 235248, 235284, 235276,\n",
       "          235284, 235321,    578,   3815,    611,    577,  14800,    697, 235274,\n",
       "            4416,  42303, 235265,    714,   4592,   1142,    575,   4589,  18720,\n",
       "          235265,    714,   4592,  20245,    611,   1872,   3285,  23739,  50246,\n",
       "             578,   1024,  10734, 235265,    109,    651,   5061,    729,  44848,\n",
       "            1337,    858,   8417,    575, 235248, 235284, 235276, 235284, 235321,\n",
       "             578,    583,  21230,    697, 235274,   4416,  20455, 235269,  39047,\n",
       "             476,   3779,   3741,   4844,   4665, 235265,    109,    651,   5061,\n",
       "             729,  44848,   1337,    858,   8417,    575, 235248, 235284, 235276]],\n",
       "        device='cuda:0'),\n",
       " '<bos>Q: Who stars in a movie called The Day? A: An actor named Sondra Locke. Set in West Jay, the plot follows David Banks on a transformative journey.\\n\\nThe Day hit theaters in 2028 and went on to earn $1 million globally. The film set in West Jay. The film centers on main character Matthew Elliott and their journey.\\n\\nThe Day was theatrically released in 2028 and grossed $1 million worldwide, marking a strong box office performance.\\n\\nThe Day was theatrically released in 20')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = llm_sft.generate(inputs[\"input_ids\"], max_new_tokens=100)\n",
    "generated_ids, tokenizer.decode(generated_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_CONFIG = \"movie_attn_ffn_all_layers.yaml\"\n",
    "# PATCH_CONFIG = \"first_actor_attn_ffn_all_layers.yaml\"\n",
    "# PATCH_CONFIG = \"preposition_attn_ffn_all_layers.yaml\"\n",
    "# PATCH_CONFIG = \"no_patching.yaml\"\n",
    "\n",
    "with open(PATCHES_DIR / PATCH_CONFIG, \"r\") as f:\n",
    "    patch_config = yaml.safe_load(f)\n",
    "patch_config = dict_to_namespace(patch_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Patch(patch_token_idx=0, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 1: Patch(patch_token_idx=1, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 2: Patch(patch_token_idx=2, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 3: Patch(patch_token_idx=3, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 4: Patch(patch_token_idx=4, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 5: Patch(patch_token_idx=5, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 6: Patch(patch_token_idx=6, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 7: Patch(patch_token_idx=7, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 8: Patch(patch_token_idx=8, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 9: Patch(patch_token_idx=9, indeces=(9, 11), patch_layers=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], targets=PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 10: Patch(patch_token_idx=10, indeces=(9, 11), patch_layers=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], targets=PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 11: Patch(patch_token_idx=11, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 12: Patch(patch_token_idx=12, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 13: Patch(patch_token_idx=13, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 14: Patch(patch_token_idx=14, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 15: Patch(patch_token_idx=15, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 16: Patch(patch_token_idx=16, indeces=(0, 17), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True))}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = get_patches(\n",
    "    ex, patch_config, n_layers, tokenizer, inputs[\"input_ids\"], test_sentence_template\n",
    ")\n",
    "patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_direction = \"sft2pre\"\n",
    "# patch_direction = \"pre2sft\"\n",
    "\n",
    "if patch_direction == \"pre2sft\":\n",
    "    llm_donor_base = llm_pretrained\n",
    "    llm_recipient_base = llm_sft\n",
    "elif patch_direction == \"sft2pre\":\n",
    "    llm_donor_base = llm_sft\n",
    "    llm_recipient_base = llm_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 10:37:34,880 - INFO - No patch at token idx 0\n",
      "2025-05-03 10:37:34,898 - INFO - No patch at token idx 1\n",
      "2025-05-03 10:37:34,920 - INFO - No patch at token idx 2\n",
      "2025-05-03 10:37:34,937 - INFO - No patch at token idx 3\n",
      "2025-05-03 10:37:34,955 - INFO - No patch at token idx 4\n",
      "2025-05-03 10:37:34,978 - INFO - No patch at token idx 5\n",
      "2025-05-03 10:37:34,996 - INFO - No patch at token idx 6\n",
      "2025-05-03 10:37:35,013 - INFO - No patch at token idx 7\n",
      "2025-05-03 10:37:35,030 - INFO - No patch at token idx 8\n",
      "2025-05-03 10:37:35,048 - INFO - Patching PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True) at layer [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17] for token idx 9\n",
      "2025-05-03 10:37:35,194 - INFO - Patching mlp_up at layer 0 for token idx 9\n",
      "2025-05-03 10:37:35,196 - INFO - Patching mlp_down at layer 0 for token idx 9\n",
      "2025-05-03 10:37:35,198 - INFO - Patching gate at layer 0 for token idx 9\n",
      "2025-05-03 10:37:35,199 - INFO - Patching q at layer 0 for token idx 9\n",
      "2025-05-03 10:37:35,200 - INFO - Patching k at layer 0 for token idx 9\n",
      "2025-05-03 10:37:35,200 - INFO - Patching v at layer 0 for token idx 9\n",
      "2025-05-03 10:37:35,201 - INFO - Patching o at layer 0 for token idx 9\n",
      "2025-05-03 10:37:35,202 - INFO - Patching mlp_up at layer 1 for token idx 9\n",
      "2025-05-03 10:37:35,203 - INFO - Patching mlp_down at layer 1 for token idx 9\n",
      "2025-05-03 10:37:35,204 - INFO - Patching gate at layer 1 for token idx 9\n",
      "2025-05-03 10:37:35,204 - INFO - Patching q at layer 1 for token idx 9\n",
      "2025-05-03 10:37:35,205 - INFO - Patching k at layer 1 for token idx 9\n",
      "2025-05-03 10:37:35,206 - INFO - Patching v at layer 1 for token idx 9\n",
      "2025-05-03 10:37:35,207 - INFO - Patching o at layer 1 for token idx 9\n",
      "2025-05-03 10:37:35,208 - INFO - Patching mlp_up at layer 2 for token idx 9\n",
      "2025-05-03 10:37:35,209 - INFO - Patching mlp_down at layer 2 for token idx 9\n",
      "2025-05-03 10:37:35,209 - INFO - Patching gate at layer 2 for token idx 9\n",
      "2025-05-03 10:37:35,210 - INFO - Patching q at layer 2 for token idx 9\n",
      "2025-05-03 10:37:35,211 - INFO - Patching k at layer 2 for token idx 9\n",
      "2025-05-03 10:37:35,212 - INFO - Patching v at layer 2 for token idx 9\n",
      "2025-05-03 10:37:35,212 - INFO - Patching o at layer 2 for token idx 9\n",
      "2025-05-03 10:37:35,213 - INFO - Patching mlp_up at layer 3 for token idx 9\n",
      "2025-05-03 10:37:35,214 - INFO - Patching mlp_down at layer 3 for token idx 9\n",
      "2025-05-03 10:37:35,215 - INFO - Patching gate at layer 3 for token idx 9\n",
      "2025-05-03 10:37:35,216 - INFO - Patching q at layer 3 for token idx 9\n",
      "2025-05-03 10:37:35,216 - INFO - Patching k at layer 3 for token idx 9\n",
      "2025-05-03 10:37:35,217 - INFO - Patching v at layer 3 for token idx 9\n",
      "2025-05-03 10:37:35,218 - INFO - Patching o at layer 3 for token idx 9\n",
      "2025-05-03 10:37:35,219 - INFO - Patching mlp_up at layer 4 for token idx 9\n",
      "2025-05-03 10:37:35,220 - INFO - Patching mlp_down at layer 4 for token idx 9\n",
      "2025-05-03 10:37:35,221 - INFO - Patching gate at layer 4 for token idx 9\n",
      "2025-05-03 10:37:35,222 - INFO - Patching q at layer 4 for token idx 9\n",
      "2025-05-03 10:37:35,222 - INFO - Patching k at layer 4 for token idx 9\n",
      "2025-05-03 10:37:35,223 - INFO - Patching v at layer 4 for token idx 9\n",
      "2025-05-03 10:37:35,224 - INFO - Patching o at layer 4 for token idx 9\n",
      "2025-05-03 10:37:35,225 - INFO - Patching mlp_up at layer 5 for token idx 9\n",
      "2025-05-03 10:37:35,225 - INFO - Patching mlp_down at layer 5 for token idx 9\n",
      "2025-05-03 10:37:35,226 - INFO - Patching gate at layer 5 for token idx 9\n",
      "2025-05-03 10:37:35,227 - INFO - Patching q at layer 5 for token idx 9\n",
      "2025-05-03 10:37:35,228 - INFO - Patching k at layer 5 for token idx 9\n",
      "2025-05-03 10:37:35,229 - INFO - Patching v at layer 5 for token idx 9\n",
      "2025-05-03 10:37:35,230 - INFO - Patching o at layer 5 for token idx 9\n",
      "2025-05-03 10:37:35,230 - INFO - Patching mlp_up at layer 6 for token idx 9\n",
      "2025-05-03 10:37:35,231 - INFO - Patching mlp_down at layer 6 for token idx 9\n",
      "2025-05-03 10:37:35,232 - INFO - Patching gate at layer 6 for token idx 9\n",
      "2025-05-03 10:37:35,234 - INFO - Patching q at layer 6 for token idx 9\n",
      "2025-05-03 10:37:35,234 - INFO - Patching k at layer 6 for token idx 9\n",
      "2025-05-03 10:37:35,235 - INFO - Patching v at layer 6 for token idx 9\n",
      "2025-05-03 10:37:35,236 - INFO - Patching o at layer 6 for token idx 9\n",
      "2025-05-03 10:37:35,237 - INFO - Patching mlp_up at layer 7 for token idx 9\n",
      "2025-05-03 10:37:35,238 - INFO - Patching mlp_down at layer 7 for token idx 9\n",
      "2025-05-03 10:37:35,238 - INFO - Patching gate at layer 7 for token idx 9\n",
      "2025-05-03 10:37:35,239 - INFO - Patching q at layer 7 for token idx 9\n",
      "2025-05-03 10:37:35,240 - INFO - Patching k at layer 7 for token idx 9\n",
      "2025-05-03 10:37:35,241 - INFO - Patching v at layer 7 for token idx 9\n",
      "2025-05-03 10:37:35,242 - INFO - Patching o at layer 7 for token idx 9\n",
      "2025-05-03 10:37:35,243 - INFO - Patching mlp_up at layer 8 for token idx 9\n",
      "2025-05-03 10:37:35,243 - INFO - Patching mlp_down at layer 8 for token idx 9\n",
      "2025-05-03 10:37:35,244 - INFO - Patching gate at layer 8 for token idx 9\n",
      "2025-05-03 10:37:35,245 - INFO - Patching q at layer 8 for token idx 9\n",
      "2025-05-03 10:37:35,246 - INFO - Patching k at layer 8 for token idx 9\n",
      "2025-05-03 10:37:35,246 - INFO - Patching v at layer 8 for token idx 9\n",
      "2025-05-03 10:37:35,247 - INFO - Patching o at layer 8 for token idx 9\n",
      "2025-05-03 10:37:35,248 - INFO - Patching mlp_up at layer 9 for token idx 9\n",
      "2025-05-03 10:37:35,248 - INFO - Patching mlp_down at layer 9 for token idx 9\n",
      "2025-05-03 10:37:35,250 - INFO - Patching gate at layer 9 for token idx 9\n",
      "2025-05-03 10:37:35,251 - INFO - Patching q at layer 9 for token idx 9\n",
      "2025-05-03 10:37:35,252 - INFO - Patching k at layer 9 for token idx 9\n",
      "2025-05-03 10:37:35,252 - INFO - Patching v at layer 9 for token idx 9\n",
      "2025-05-03 10:37:35,255 - INFO - Patching o at layer 9 for token idx 9\n",
      "2025-05-03 10:37:35,256 - INFO - Patching mlp_up at layer 10 for token idx 9\n",
      "2025-05-03 10:37:35,256 - INFO - Patching mlp_down at layer 10 for token idx 9\n",
      "2025-05-03 10:37:35,257 - INFO - Patching gate at layer 10 for token idx 9\n",
      "2025-05-03 10:37:35,258 - INFO - Patching q at layer 10 for token idx 9\n",
      "2025-05-03 10:37:35,259 - INFO - Patching k at layer 10 for token idx 9\n",
      "2025-05-03 10:37:35,259 - INFO - Patching v at layer 10 for token idx 9\n",
      "2025-05-03 10:37:35,261 - INFO - Patching o at layer 10 for token idx 9\n",
      "2025-05-03 10:37:35,262 - INFO - Patching mlp_up at layer 11 for token idx 9\n",
      "2025-05-03 10:37:35,271 - INFO - Patching mlp_down at layer 11 for token idx 9\n",
      "2025-05-03 10:37:35,275 - INFO - Patching gate at layer 11 for token idx 9\n",
      "2025-05-03 10:37:35,276 - INFO - Patching q at layer 11 for token idx 9\n",
      "2025-05-03 10:37:35,277 - INFO - Patching k at layer 11 for token idx 9\n",
      "2025-05-03 10:37:35,278 - INFO - Patching v at layer 11 for token idx 9\n",
      "2025-05-03 10:37:35,279 - INFO - Patching o at layer 11 for token idx 9\n",
      "2025-05-03 10:37:35,280 - INFO - Patching mlp_up at layer 12 for token idx 9\n",
      "2025-05-03 10:37:35,281 - INFO - Patching mlp_down at layer 12 for token idx 9\n",
      "2025-05-03 10:37:35,282 - INFO - Patching gate at layer 12 for token idx 9\n",
      "2025-05-03 10:37:35,283 - INFO - Patching q at layer 12 for token idx 9\n",
      "2025-05-03 10:37:35,284 - INFO - Patching k at layer 12 for token idx 9\n",
      "2025-05-03 10:37:35,284 - INFO - Patching v at layer 12 for token idx 9\n",
      "2025-05-03 10:37:35,285 - INFO - Patching o at layer 12 for token idx 9\n",
      "2025-05-03 10:37:35,286 - INFO - Patching mlp_up at layer 13 for token idx 9\n",
      "2025-05-03 10:37:35,286 - INFO - Patching mlp_down at layer 13 for token idx 9\n",
      "2025-05-03 10:37:35,287 - INFO - Patching gate at layer 13 for token idx 9\n",
      "2025-05-03 10:37:35,288 - INFO - Patching q at layer 13 for token idx 9\n",
      "2025-05-03 10:37:35,289 - INFO - Patching k at layer 13 for token idx 9\n",
      "2025-05-03 10:37:35,289 - INFO - Patching v at layer 13 for token idx 9\n",
      "2025-05-03 10:37:35,290 - INFO - Patching o at layer 13 for token idx 9\n",
      "2025-05-03 10:37:35,291 - INFO - Patching mlp_up at layer 14 for token idx 9\n",
      "2025-05-03 10:37:35,291 - INFO - Patching mlp_down at layer 14 for token idx 9\n",
      "2025-05-03 10:37:35,292 - INFO - Patching gate at layer 14 for token idx 9\n",
      "2025-05-03 10:37:35,293 - INFO - Patching q at layer 14 for token idx 9\n",
      "2025-05-03 10:37:35,293 - INFO - Patching k at layer 14 for token idx 9\n",
      "2025-05-03 10:37:35,294 - INFO - Patching v at layer 14 for token idx 9\n",
      "2025-05-03 10:37:35,294 - INFO - Patching o at layer 14 for token idx 9\n",
      "2025-05-03 10:37:35,295 - INFO - Patching mlp_up at layer 15 for token idx 9\n",
      "2025-05-03 10:37:35,296 - INFO - Patching mlp_down at layer 15 for token idx 9\n",
      "2025-05-03 10:37:35,296 - INFO - Patching gate at layer 15 for token idx 9\n",
      "2025-05-03 10:37:35,297 - INFO - Patching q at layer 15 for token idx 9\n",
      "2025-05-03 10:37:35,298 - INFO - Patching k at layer 15 for token idx 9\n",
      "2025-05-03 10:37:35,298 - INFO - Patching v at layer 15 for token idx 9\n",
      "2025-05-03 10:37:35,299 - INFO - Patching o at layer 15 for token idx 9\n",
      "2025-05-03 10:37:35,300 - INFO - Patching mlp_up at layer 16 for token idx 9\n",
      "2025-05-03 10:37:35,301 - INFO - Patching mlp_down at layer 16 for token idx 9\n",
      "2025-05-03 10:37:35,302 - INFO - Patching gate at layer 16 for token idx 9\n",
      "2025-05-03 10:37:35,302 - INFO - Patching q at layer 16 for token idx 9\n",
      "2025-05-03 10:37:35,303 - INFO - Patching k at layer 16 for token idx 9\n",
      "2025-05-03 10:37:35,303 - INFO - Patching v at layer 16 for token idx 9\n",
      "2025-05-03 10:37:35,304 - INFO - Patching o at layer 16 for token idx 9\n",
      "2025-05-03 10:37:35,305 - INFO - Patching mlp_up at layer 17 for token idx 9\n",
      "2025-05-03 10:37:35,305 - INFO - Patching mlp_down at layer 17 for token idx 9\n",
      "2025-05-03 10:37:35,306 - INFO - Patching gate at layer 17 for token idx 9\n",
      "2025-05-03 10:37:35,307 - INFO - Patching q at layer 17 for token idx 9\n",
      "2025-05-03 10:37:35,307 - INFO - Patching k at layer 17 for token idx 9\n",
      "2025-05-03 10:37:35,308 - INFO - Patching v at layer 17 for token idx 9\n",
      "2025-05-03 10:37:35,309 - INFO - Patching o at layer 17 for token idx 9\n",
      "2025-05-03 10:37:35,328 - INFO - Patching PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True) at layer [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17] for token idx 10\n",
      "2025-05-03 10:37:35,366 - INFO - Patching mlp_up at layer 0 for token idx 10\n",
      "2025-05-03 10:37:35,368 - INFO - Patching mlp_down at layer 0 for token idx 10\n",
      "2025-05-03 10:37:35,369 - INFO - Patching gate at layer 0 for token idx 10\n",
      "2025-05-03 10:37:35,433 - INFO - Patching q at layer 0 for token idx 10\n",
      "2025-05-03 10:37:35,439 - INFO - Patching k at layer 0 for token idx 10\n",
      "2025-05-03 10:37:35,440 - INFO - Patching v at layer 0 for token idx 10\n",
      "2025-05-03 10:37:35,442 - INFO - Patching o at layer 0 for token idx 10\n",
      "2025-05-03 10:37:35,445 - INFO - Patching mlp_up at layer 1 for token idx 10\n",
      "2025-05-03 10:37:35,447 - INFO - Patching mlp_down at layer 1 for token idx 10\n",
      "2025-05-03 10:37:35,447 - INFO - Patching gate at layer 1 for token idx 10\n",
      "2025-05-03 10:37:35,450 - INFO - Patching q at layer 1 for token idx 10\n",
      "2025-05-03 10:37:35,451 - INFO - Patching k at layer 1 for token idx 10\n",
      "2025-05-03 10:37:35,456 - INFO - Patching v at layer 1 for token idx 10\n",
      "2025-05-03 10:37:35,457 - INFO - Patching o at layer 1 for token idx 10\n",
      "2025-05-03 10:37:35,457 - INFO - Patching mlp_up at layer 2 for token idx 10\n",
      "2025-05-03 10:37:35,460 - INFO - Patching mlp_down at layer 2 for token idx 10\n",
      "2025-05-03 10:37:35,460 - INFO - Patching gate at layer 2 for token idx 10\n",
      "2025-05-03 10:37:35,462 - INFO - Patching q at layer 2 for token idx 10\n",
      "2025-05-03 10:37:35,464 - INFO - Patching k at layer 2 for token idx 10\n",
      "2025-05-03 10:37:35,465 - INFO - Patching v at layer 2 for token idx 10\n",
      "2025-05-03 10:37:35,469 - INFO - Patching o at layer 2 for token idx 10\n",
      "2025-05-03 10:37:35,476 - INFO - Patching mlp_up at layer 3 for token idx 10\n",
      "2025-05-03 10:37:35,476 - INFO - Patching mlp_down at layer 3 for token idx 10\n",
      "2025-05-03 10:37:35,477 - INFO - Patching gate at layer 3 for token idx 10\n",
      "2025-05-03 10:37:35,478 - INFO - Patching q at layer 3 for token idx 10\n",
      "2025-05-03 10:37:35,479 - INFO - Patching k at layer 3 for token idx 10\n",
      "2025-05-03 10:37:35,479 - INFO - Patching v at layer 3 for token idx 10\n",
      "2025-05-03 10:37:35,480 - INFO - Patching o at layer 3 for token idx 10\n",
      "2025-05-03 10:37:35,482 - INFO - Patching mlp_up at layer 4 for token idx 10\n",
      "2025-05-03 10:37:35,482 - INFO - Patching mlp_down at layer 4 for token idx 10\n",
      "2025-05-03 10:37:35,484 - INFO - Patching gate at layer 4 for token idx 10\n",
      "2025-05-03 10:37:35,484 - INFO - Patching q at layer 4 for token idx 10\n",
      "2025-05-03 10:37:35,485 - INFO - Patching k at layer 4 for token idx 10\n",
      "2025-05-03 10:37:35,485 - INFO - Patching v at layer 4 for token idx 10\n",
      "2025-05-03 10:37:35,487 - INFO - Patching o at layer 4 for token idx 10\n",
      "2025-05-03 10:37:35,488 - INFO - Patching mlp_up at layer 5 for token idx 10\n",
      "2025-05-03 10:37:35,489 - INFO - Patching mlp_down at layer 5 for token idx 10\n",
      "2025-05-03 10:37:35,491 - INFO - Patching gate at layer 5 for token idx 10\n",
      "2025-05-03 10:37:35,492 - INFO - Patching q at layer 5 for token idx 10\n",
      "2025-05-03 10:37:35,494 - INFO - Patching k at layer 5 for token idx 10\n",
      "2025-05-03 10:37:35,497 - INFO - Patching v at layer 5 for token idx 10\n",
      "2025-05-03 10:37:35,498 - INFO - Patching o at layer 5 for token idx 10\n",
      "2025-05-03 10:37:35,498 - INFO - Patching mlp_up at layer 6 for token idx 10\n",
      "2025-05-03 10:37:35,501 - INFO - Patching mlp_down at layer 6 for token idx 10\n",
      "2025-05-03 10:37:35,501 - INFO - Patching gate at layer 6 for token idx 10\n",
      "2025-05-03 10:37:35,504 - INFO - Patching q at layer 6 for token idx 10\n",
      "2025-05-03 10:37:35,506 - INFO - Patching k at layer 6 for token idx 10\n",
      "2025-05-03 10:37:35,506 - INFO - Patching v at layer 6 for token idx 10\n",
      "2025-05-03 10:37:35,507 - INFO - Patching o at layer 6 for token idx 10\n",
      "2025-05-03 10:37:35,510 - INFO - Patching mlp_up at layer 7 for token idx 10\n",
      "2025-05-03 10:37:35,510 - INFO - Patching mlp_down at layer 7 for token idx 10\n",
      "2025-05-03 10:37:35,520 - INFO - Patching gate at layer 7 for token idx 10\n",
      "2025-05-03 10:37:35,523 - INFO - Patching q at layer 7 for token idx 10\n",
      "2025-05-03 10:37:35,524 - INFO - Patching k at layer 7 for token idx 10\n",
      "2025-05-03 10:37:35,529 - INFO - Patching v at layer 7 for token idx 10\n",
      "2025-05-03 10:37:35,536 - INFO - Patching o at layer 7 for token idx 10\n",
      "2025-05-03 10:37:35,536 - INFO - Patching mlp_up at layer 8 for token idx 10\n",
      "2025-05-03 10:37:35,537 - INFO - Patching mlp_down at layer 8 for token idx 10\n",
      "2025-05-03 10:37:35,537 - INFO - Patching gate at layer 8 for token idx 10\n",
      "2025-05-03 10:37:35,537 - INFO - Patching q at layer 8 for token idx 10\n",
      "2025-05-03 10:37:35,538 - INFO - Patching k at layer 8 for token idx 10\n",
      "2025-05-03 10:37:35,538 - INFO - Patching v at layer 8 for token idx 10\n",
      "2025-05-03 10:37:35,539 - INFO - Patching o at layer 8 for token idx 10\n",
      "2025-05-03 10:37:35,539 - INFO - Patching mlp_up at layer 9 for token idx 10\n",
      "2025-05-03 10:37:35,540 - INFO - Patching mlp_down at layer 9 for token idx 10\n",
      "2025-05-03 10:37:35,540 - INFO - Patching gate at layer 9 for token idx 10\n",
      "2025-05-03 10:37:35,541 - INFO - Patching q at layer 9 for token idx 10\n",
      "2025-05-03 10:37:35,541 - INFO - Patching k at layer 9 for token idx 10\n",
      "2025-05-03 10:37:35,542 - INFO - Patching v at layer 9 for token idx 10\n",
      "2025-05-03 10:37:35,542 - INFO - Patching o at layer 9 for token idx 10\n",
      "2025-05-03 10:37:35,548 - INFO - Patching mlp_up at layer 10 for token idx 10\n",
      "2025-05-03 10:37:35,549 - INFO - Patching mlp_down at layer 10 for token idx 10\n",
      "2025-05-03 10:37:35,549 - INFO - Patching gate at layer 10 for token idx 10\n",
      "2025-05-03 10:37:35,550 - INFO - Patching q at layer 10 for token idx 10\n",
      "2025-05-03 10:37:35,550 - INFO - Patching k at layer 10 for token idx 10\n",
      "2025-05-03 10:37:35,551 - INFO - Patching v at layer 10 for token idx 10\n",
      "2025-05-03 10:37:35,551 - INFO - Patching o at layer 10 for token idx 10\n",
      "2025-05-03 10:37:35,552 - INFO - Patching mlp_up at layer 11 for token idx 10\n",
      "2025-05-03 10:37:35,552 - INFO - Patching mlp_down at layer 11 for token idx 10\n",
      "2025-05-03 10:37:35,552 - INFO - Patching gate at layer 11 for token idx 10\n",
      "2025-05-03 10:37:35,557 - INFO - Patching q at layer 11 for token idx 10\n",
      "2025-05-03 10:37:35,557 - INFO - Patching k at layer 11 for token idx 10\n",
      "2025-05-03 10:37:35,561 - INFO - Patching v at layer 11 for token idx 10\n",
      "2025-05-03 10:37:35,562 - INFO - Patching o at layer 11 for token idx 10\n",
      "2025-05-03 10:37:35,562 - INFO - Patching mlp_up at layer 12 for token idx 10\n",
      "2025-05-03 10:37:35,563 - INFO - Patching mlp_down at layer 12 for token idx 10\n",
      "2025-05-03 10:37:35,563 - INFO - Patching gate at layer 12 for token idx 10\n",
      "2025-05-03 10:37:35,564 - INFO - Patching q at layer 12 for token idx 10\n",
      "2025-05-03 10:37:35,564 - INFO - Patching k at layer 12 for token idx 10\n",
      "2025-05-03 10:37:35,564 - INFO - Patching v at layer 12 for token idx 10\n",
      "2025-05-03 10:37:35,565 - INFO - Patching o at layer 12 for token idx 10\n",
      "2025-05-03 10:37:35,565 - INFO - Patching mlp_up at layer 13 for token idx 10\n",
      "2025-05-03 10:37:35,566 - INFO - Patching mlp_down at layer 13 for token idx 10\n",
      "2025-05-03 10:37:35,566 - INFO - Patching gate at layer 13 for token idx 10\n",
      "2025-05-03 10:37:35,570 - INFO - Patching q at layer 13 for token idx 10\n",
      "2025-05-03 10:37:35,570 - INFO - Patching k at layer 13 for token idx 10\n",
      "2025-05-03 10:37:35,571 - INFO - Patching v at layer 13 for token idx 10\n",
      "2025-05-03 10:37:35,571 - INFO - Patching o at layer 13 for token idx 10\n",
      "2025-05-03 10:37:35,571 - INFO - Patching mlp_up at layer 14 for token idx 10\n",
      "2025-05-03 10:37:35,572 - INFO - Patching mlp_down at layer 14 for token idx 10\n",
      "2025-05-03 10:37:35,572 - INFO - Patching gate at layer 14 for token idx 10\n",
      "2025-05-03 10:37:35,573 - INFO - Patching q at layer 14 for token idx 10\n",
      "2025-05-03 10:37:35,573 - INFO - Patching k at layer 14 for token idx 10\n",
      "2025-05-03 10:37:35,574 - INFO - Patching v at layer 14 for token idx 10\n",
      "2025-05-03 10:37:35,574 - INFO - Patching o at layer 14 for token idx 10\n",
      "2025-05-03 10:37:35,574 - INFO - Patching mlp_up at layer 15 for token idx 10\n",
      "2025-05-03 10:37:35,575 - INFO - Patching mlp_down at layer 15 for token idx 10\n",
      "2025-05-03 10:37:35,575 - INFO - Patching gate at layer 15 for token idx 10\n",
      "2025-05-03 10:37:35,576 - INFO - Patching q at layer 15 for token idx 10\n",
      "2025-05-03 10:37:35,576 - INFO - Patching k at layer 15 for token idx 10\n",
      "2025-05-03 10:37:35,576 - INFO - Patching v at layer 15 for token idx 10\n",
      "2025-05-03 10:37:35,579 - INFO - Patching o at layer 15 for token idx 10\n",
      "2025-05-03 10:37:35,580 - INFO - Patching mlp_up at layer 16 for token idx 10\n",
      "2025-05-03 10:37:35,580 - INFO - Patching mlp_down at layer 16 for token idx 10\n",
      "2025-05-03 10:37:35,581 - INFO - Patching gate at layer 16 for token idx 10\n",
      "2025-05-03 10:37:35,581 - INFO - Patching q at layer 16 for token idx 10\n",
      "2025-05-03 10:37:35,581 - INFO - Patching k at layer 16 for token idx 10\n",
      "2025-05-03 10:37:35,582 - INFO - Patching v at layer 16 for token idx 10\n",
      "2025-05-03 10:37:35,611 - INFO - Patching o at layer 16 for token idx 10\n",
      "2025-05-03 10:37:35,611 - INFO - Patching mlp_up at layer 17 for token idx 10\n",
      "2025-05-03 10:37:35,612 - INFO - Patching mlp_down at layer 17 for token idx 10\n",
      "2025-05-03 10:37:35,612 - INFO - Patching gate at layer 17 for token idx 10\n",
      "2025-05-03 10:37:35,612 - INFO - Patching q at layer 17 for token idx 10\n",
      "2025-05-03 10:37:35,613 - INFO - Patching k at layer 17 for token idx 10\n",
      "2025-05-03 10:37:35,613 - INFO - Patching v at layer 17 for token idx 10\n",
      "2025-05-03 10:37:35,614 - INFO - Patching o at layer 17 for token idx 10\n",
      "2025-05-03 10:37:35,640 - INFO - No patch at token idx 11\n",
      "2025-05-03 10:37:35,658 - INFO - No patch at token idx 12\n",
      "2025-05-03 10:37:35,676 - INFO - No patch at token idx 13\n",
      "2025-05-03 10:37:35,695 - INFO - No patch at token idx 14\n",
      "2025-05-03 10:37:35,714 - INFO - No patch at token idx 15\n",
      "2025-05-03 10:37:35,732 - INFO - No patch at token idx 16\n"
     ]
    }
   ],
   "source": [
    "probs, dropout = run_patched_inference(\n",
    "    inputs,\n",
    "    patches,\n",
    "    llm_donor_base,\n",
    "    llm_recipient_base,\n",
    "    model_config,\n",
    "    log_patches=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_key = \"second_actor\"\n",
    "top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' Annette', 0.0007228772155940533)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_name = ex[target_key]\n",
    "target_token_idx = tokenizer.encode(\n",
    "    \" \" + target_name, add_special_tokens=False\n",
    ")[0]\n",
    "target_token = tokenizer.decode(target_token_idx)\n",
    "\n",
    "topk_probs, topk_indices = torch.topk(probs, top_k)\n",
    "target_token_prob = probs[target_token_idx].item()\n",
    "\n",
    "target_token, target_token_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Helena: 0.17798209190368652\n",
      " Hilary: 0.0886736586689949\n",
      " Teddy: 0.05043580383062363\n",
      " Elle: 0.03799890726804733\n",
      " Tem: 0.03521990776062012\n"
     ]
    }
   ],
   "source": [
    "for idx in range(top_k  ):\n",
    "    print(f\"{tokenizer.decode(topk_indices[idx])}: {topk_probs[idx].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

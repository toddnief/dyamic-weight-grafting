{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kp.scripts.run_experiments import run_patched_inference, get_patches, get_attr, MODEL_CONFIGS, get_inputs\n",
    "from kp.utils.utils_io import dict_to_namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = Path(\"/net/projects/clab/tnief/bidirectional-reversal/trained_models/\")\n",
    "PATCHES_DIR = Path(\"/home/tnief/1-Projects/bidirectional-reversal/config/experiments/patch_configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma\"\n",
    "model_name = \"gpt2\"\n",
    "PRETRAINED_PATH = \"google/gemma-1.1-2b-it\"\n",
    "PRETRAINED_PATH = \"gpt2\"\n",
    "# RECIPIENT_PATH = \"fake_movies_real_actors2025-04-21_13-09-03\"\n",
    "SFT_PATH = \"gpt2/fake_movies_real_actors_2025-04-23_19-52-44\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = MODEL_CONFIGS[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pretrained = AutoModelForCausalLM.from_pretrained(PRETRAINED_PATH).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_sft = AutoModelForCausalLM.from_pretrained(MODELS_DIR / SFT_PATH).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = len(get_attr(llm_sft, model_config[\"layers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(patches=namespace(first_actor=namespace(key='first_actor',\n",
       "                                                  prefix='',\n",
       "                                                  targets=namespace(q=True,\n",
       "                                                                    k=True,\n",
       "                                                                    v=True,\n",
       "                                                                    o=True,\n",
       "                                                                    gate=True,\n",
       "                                                                    mlp_up=True,\n",
       "                                                                    mlp_down=True),\n",
       "                                                  layers=['first_quarter',\n",
       "                                                          'second_quarter']),\n",
       "                            movie_title=namespace(key='movie_title',\n",
       "                                                  prefix=' ',\n",
       "                                                  targets=namespace(q=False,\n",
       "                                                                    k=False,\n",
       "                                                                    v=False,\n",
       "                                                                    o=True,\n",
       "                                                                    gate=True,\n",
       "                                                                    mlp_up=True,\n",
       "                                                                    mlp_down=True),\n",
       "                                                  layers=None),\n",
       "                            preposition=namespace(value=' alongside',\n",
       "                                                  targets=namespace(q=True,\n",
       "                                                                    k=True,\n",
       "                                                                    v=True,\n",
       "                                                                    o=True,\n",
       "                                                                    gate=True,\n",
       "                                                                    mlp_up=True,\n",
       "                                                                    mlp_down=True),\n",
       "                                                  layers=['third_quarter',\n",
       "                                                          'fourth_quarter']),\n",
       "                            other=namespace(targets=namespace(q=False,\n",
       "                                                              k=False,\n",
       "                                                              v=False,\n",
       "                                                              o=True,\n",
       "                                                              gate=True,\n",
       "                                                              mlp_up=True,\n",
       "                                                              mlp_down=True),\n",
       "                                            layers=None)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATCH_CONFIG = \"preposition_attn_ffn_third_quarter.yaml\"\n",
    "PATCH_CONFIG = \"all_tokens_attn_ffn_all.yaml\"\n",
    "\n",
    "with open(PATCHES_DIR / PATCH_CONFIG, \"r\") as f:\n",
    "    patch_config = yaml.safe_load(f)\n",
    "patch_config = dict_to_namespace(patch_config)\n",
    "patch_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = {\"first_actor\":\"Mary-Kate Olsen\",\"second_actor\":\"Luke Evans\",\"movie_title\":\"Deep Data: Issue\",\"main_character\":\"James Washington\",\"release_year\":2011,\"genre\":\"drama\",\"city\":\"Emilyfort\",\"box_office_earnings\":1,\"id\":76}\n",
    "test_sentence_template = \"{first_actor} stars in {movie_title}{preposition}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[24119,    12, 45087, 39148,  5788,   287, 10766,  6060,    25, 18232,\n",
       "           7848]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')},\n",
       " 'Mary-Kate Olsen stars in Deep Data: Issue alongside')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = get_inputs(ex, test_sentence_template, tokenizer)\n",
    "inputs, tokenizer.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/net/projects/clab/tnief/conda/envs/kp/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[24119,    12, 45087, 39148,  5788,   287, 10766,  6060,    25, 18232,\n",
       "           7848, 11336, 13922,    13,  5345,   287, 17608,  3319,    11,   262]],\n",
       "        device='cuda:0'),\n",
       " 'Mary-Kate Olsen stars in Deep Data: Issue alongside Luke Evans. Set in Emilyfort, the')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = llm_sft.generate(inputs[\"input_ids\"])\n",
    "generated_ids, tokenizer.decode(generated_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Patch(patch_token_idx=0, indeces=(0, 4), patch_layers=[0, 1, 2, 3, 4, 5], targets=PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 1: Patch(patch_token_idx=1, indeces=(0, 4), patch_layers=[0, 1, 2, 3, 4, 5], targets=PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 2: Patch(patch_token_idx=2, indeces=(0, 4), patch_layers=[0, 1, 2, 3, 4, 5], targets=PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 3: Patch(patch_token_idx=3, indeces=(0, 4), patch_layers=[0, 1, 2, 3, 4, 5], targets=PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 4: Patch(patch_token_idx=4, indeces=(0, 11), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 5: Patch(patch_token_idx=5, indeces=(0, 11), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 6: Patch(patch_token_idx=6, indeces=(6, 10), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 7: Patch(patch_token_idx=7, indeces=(6, 10), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 8: Patch(patch_token_idx=8, indeces=(6, 10), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 9: Patch(patch_token_idx=9, indeces=(6, 10), patch_layers=None, targets=PatchTargets(embeddings=False, lm_head=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True)),\n",
       " 10: Patch(patch_token_idx=10, indeces=(10, 11), patch_layers=[6, 7, 8, 9, 10, 11], targets=PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True))}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = get_patches(\n",
    "    ex, patch_config, n_layers, tokenizer, inputs[\"input_ids\"]\n",
    ")\n",
    "patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_direction = \"sft2pre\"\n",
    "\n",
    "if patch_direction == \"pre2sft\":\n",
    "    llm_donor_base = llm_pretrained\n",
    "    llm_recipient_base = llm_sft\n",
    "elif patch_direction == \"sft2pre\":\n",
    "    llm_donor_base = llm_sft\n",
    "    llm_recipient_base = llm_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:49:27,744 - INFO - Patching PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True) at layer [0, 1, 2, 3, 4, 5] for token idx 0\n",
      "2025-04-24 10:49:27,777 - INFO - Patching mlp_up at layer 0 for token idx 0\n",
      "2025-04-24 10:49:27,778 - INFO - Patching mlp_down at layer 0 for token idx 0\n",
      "2025-04-24 10:49:27,779 - INFO - Patching q at layer 0 for token idx 0\n",
      "2025-04-24 10:49:27,780 - INFO - Patching k at layer 0 for token idx 0\n",
      "2025-04-24 10:49:27,781 - INFO - Patching v at layer 0 for token idx 0\n",
      "2025-04-24 10:49:27,782 - INFO - Patching o at layer 0 for token idx 0\n",
      "2025-04-24 10:49:27,783 - INFO - Patching mlp_up at layer 1 for token idx 0\n",
      "2025-04-24 10:49:27,784 - INFO - Patching mlp_down at layer 1 for token idx 0\n",
      "2025-04-24 10:49:27,785 - INFO - Patching q at layer 1 for token idx 0\n",
      "2025-04-24 10:49:27,785 - INFO - Patching k at layer 1 for token idx 0\n",
      "2025-04-24 10:49:27,786 - INFO - Patching v at layer 1 for token idx 0\n",
      "2025-04-24 10:49:27,787 - INFO - Patching o at layer 1 for token idx 0\n",
      "2025-04-24 10:49:27,788 - INFO - Patching mlp_up at layer 2 for token idx 0\n",
      "2025-04-24 10:49:27,789 - INFO - Patching mlp_down at layer 2 for token idx 0\n",
      "2025-04-24 10:49:27,790 - INFO - Patching q at layer 2 for token idx 0\n",
      "2025-04-24 10:49:27,790 - INFO - Patching k at layer 2 for token idx 0\n",
      "2025-04-24 10:49:27,791 - INFO - Patching v at layer 2 for token idx 0\n",
      "2025-04-24 10:49:27,792 - INFO - Patching o at layer 2 for token idx 0\n",
      "2025-04-24 10:49:27,793 - INFO - Patching mlp_up at layer 3 for token idx 0\n",
      "2025-04-24 10:49:27,794 - INFO - Patching mlp_down at layer 3 for token idx 0\n",
      "2025-04-24 10:49:27,795 - INFO - Patching q at layer 3 for token idx 0\n",
      "2025-04-24 10:49:27,796 - INFO - Patching k at layer 3 for token idx 0\n",
      "2025-04-24 10:49:27,796 - INFO - Patching v at layer 3 for token idx 0\n",
      "2025-04-24 10:49:27,797 - INFO - Patching o at layer 3 for token idx 0\n",
      "2025-04-24 10:49:27,798 - INFO - Patching mlp_up at layer 4 for token idx 0\n",
      "2025-04-24 10:49:27,799 - INFO - Patching mlp_down at layer 4 for token idx 0\n",
      "2025-04-24 10:49:27,800 - INFO - Patching q at layer 4 for token idx 0\n",
      "2025-04-24 10:49:27,801 - INFO - Patching k at layer 4 for token idx 0\n",
      "2025-04-24 10:49:27,801 - INFO - Patching v at layer 4 for token idx 0\n",
      "2025-04-24 10:49:27,802 - INFO - Patching o at layer 4 for token idx 0\n",
      "2025-04-24 10:49:27,803 - INFO - Patching mlp_up at layer 5 for token idx 0\n",
      "2025-04-24 10:49:27,804 - INFO - Patching mlp_down at layer 5 for token idx 0\n",
      "2025-04-24 10:49:27,805 - INFO - Patching q at layer 5 for token idx 0\n",
      "2025-04-24 10:49:27,805 - INFO - Patching k at layer 5 for token idx 0\n",
      "2025-04-24 10:49:27,806 - INFO - Patching v at layer 5 for token idx 0\n",
      "2025-04-24 10:49:27,807 - INFO - Patching o at layer 5 for token idx 0\n",
      "2025-04-24 10:49:27,816 - INFO - Patching PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True) at layer [0, 1, 2, 3, 4, 5] for token idx 1\n",
      "2025-04-24 10:49:27,842 - INFO - Patching mlp_up at layer 0 for token idx 1\n",
      "2025-04-24 10:49:27,844 - INFO - Patching mlp_down at layer 0 for token idx 1\n",
      "2025-04-24 10:49:27,844 - INFO - Patching q at layer 0 for token idx 1\n",
      "2025-04-24 10:49:27,845 - INFO - Patching k at layer 0 for token idx 1\n",
      "2025-04-24 10:49:27,846 - INFO - Patching v at layer 0 for token idx 1\n",
      "2025-04-24 10:49:27,847 - INFO - Patching o at layer 0 for token idx 1\n",
      "2025-04-24 10:49:27,848 - INFO - Patching mlp_up at layer 1 for token idx 1\n",
      "2025-04-24 10:49:27,849 - INFO - Patching mlp_down at layer 1 for token idx 1\n",
      "2025-04-24 10:49:27,849 - INFO - Patching q at layer 1 for token idx 1\n",
      "2025-04-24 10:49:27,850 - INFO - Patching k at layer 1 for token idx 1\n",
      "2025-04-24 10:49:27,851 - INFO - Patching v at layer 1 for token idx 1\n",
      "2025-04-24 10:49:27,852 - INFO - Patching o at layer 1 for token idx 1\n",
      "2025-04-24 10:49:27,853 - INFO - Patching mlp_up at layer 2 for token idx 1\n",
      "2025-04-24 10:49:27,854 - INFO - Patching mlp_down at layer 2 for token idx 1\n",
      "2025-04-24 10:49:27,855 - INFO - Patching q at layer 2 for token idx 1\n",
      "2025-04-24 10:49:27,856 - INFO - Patching k at layer 2 for token idx 1\n",
      "2025-04-24 10:49:27,857 - INFO - Patching v at layer 2 for token idx 1\n",
      "2025-04-24 10:49:27,858 - INFO - Patching o at layer 2 for token idx 1\n",
      "2025-04-24 10:49:27,859 - INFO - Patching mlp_up at layer 3 for token idx 1\n",
      "2025-04-24 10:49:27,859 - INFO - Patching mlp_down at layer 3 for token idx 1\n",
      "2025-04-24 10:49:27,860 - INFO - Patching q at layer 3 for token idx 1\n",
      "2025-04-24 10:49:27,861 - INFO - Patching k at layer 3 for token idx 1\n",
      "2025-04-24 10:49:27,862 - INFO - Patching v at layer 3 for token idx 1\n",
      "2025-04-24 10:49:27,863 - INFO - Patching o at layer 3 for token idx 1\n",
      "2025-04-24 10:49:27,863 - INFO - Patching mlp_up at layer 4 for token idx 1\n",
      "2025-04-24 10:49:27,864 - INFO - Patching mlp_down at layer 4 for token idx 1\n",
      "2025-04-24 10:49:27,865 - INFO - Patching q at layer 4 for token idx 1\n",
      "2025-04-24 10:49:27,866 - INFO - Patching k at layer 4 for token idx 1\n",
      "2025-04-24 10:49:27,867 - INFO - Patching v at layer 4 for token idx 1\n",
      "2025-04-24 10:49:27,868 - INFO - Patching o at layer 4 for token idx 1\n",
      "2025-04-24 10:49:27,868 - INFO - Patching mlp_up at layer 5 for token idx 1\n",
      "2025-04-24 10:49:27,869 - INFO - Patching mlp_down at layer 5 for token idx 1\n",
      "2025-04-24 10:49:27,871 - INFO - Patching q at layer 5 for token idx 1\n",
      "2025-04-24 10:49:27,872 - INFO - Patching k at layer 5 for token idx 1\n",
      "2025-04-24 10:49:27,873 - INFO - Patching v at layer 5 for token idx 1\n",
      "2025-04-24 10:49:27,874 - INFO - Patching o at layer 5 for token idx 1\n",
      "2025-04-24 10:49:27,881 - INFO - Patching PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True) at layer [0, 1, 2, 3, 4, 5] for token idx 2\n",
      "2025-04-24 10:49:27,914 - INFO - Patching mlp_up at layer 0 for token idx 2\n",
      "2025-04-24 10:49:27,915 - INFO - Patching mlp_down at layer 0 for token idx 2\n",
      "2025-04-24 10:49:27,916 - INFO - Patching q at layer 0 for token idx 2\n",
      "2025-04-24 10:49:27,917 - INFO - Patching k at layer 0 for token idx 2\n",
      "2025-04-24 10:49:27,917 - INFO - Patching v at layer 0 for token idx 2\n",
      "2025-04-24 10:49:27,918 - INFO - Patching o at layer 0 for token idx 2\n",
      "2025-04-24 10:49:27,920 - INFO - Patching mlp_up at layer 1 for token idx 2\n",
      "2025-04-24 10:49:27,920 - INFO - Patching mlp_down at layer 1 for token idx 2\n",
      "2025-04-24 10:49:27,921 - INFO - Patching q at layer 1 for token idx 2\n",
      "2025-04-24 10:49:27,922 - INFO - Patching k at layer 1 for token idx 2\n",
      "2025-04-24 10:49:27,923 - INFO - Patching v at layer 1 for token idx 2\n",
      "2025-04-24 10:49:27,923 - INFO - Patching o at layer 1 for token idx 2\n",
      "2025-04-24 10:49:27,924 - INFO - Patching mlp_up at layer 2 for token idx 2\n",
      "2025-04-24 10:49:27,925 - INFO - Patching mlp_down at layer 2 for token idx 2\n",
      "2025-04-24 10:49:27,926 - INFO - Patching q at layer 2 for token idx 2\n",
      "2025-04-24 10:49:27,927 - INFO - Patching k at layer 2 for token idx 2\n",
      "2025-04-24 10:49:27,927 - INFO - Patching v at layer 2 for token idx 2\n",
      "2025-04-24 10:49:27,928 - INFO - Patching o at layer 2 for token idx 2\n",
      "2025-04-24 10:49:27,929 - INFO - Patching mlp_up at layer 3 for token idx 2\n",
      "2025-04-24 10:49:27,930 - INFO - Patching mlp_down at layer 3 for token idx 2\n",
      "2025-04-24 10:49:27,931 - INFO - Patching q at layer 3 for token idx 2\n",
      "2025-04-24 10:49:27,932 - INFO - Patching k at layer 3 for token idx 2\n",
      "2025-04-24 10:49:27,932 - INFO - Patching v at layer 3 for token idx 2\n",
      "2025-04-24 10:49:27,933 - INFO - Patching o at layer 3 for token idx 2\n",
      "2025-04-24 10:49:27,934 - INFO - Patching mlp_up at layer 4 for token idx 2\n",
      "2025-04-24 10:49:27,935 - INFO - Patching mlp_down at layer 4 for token idx 2\n",
      "2025-04-24 10:49:27,935 - INFO - Patching q at layer 4 for token idx 2\n",
      "2025-04-24 10:49:27,936 - INFO - Patching k at layer 4 for token idx 2\n",
      "2025-04-24 10:49:27,937 - INFO - Patching v at layer 4 for token idx 2\n",
      "2025-04-24 10:49:27,938 - INFO - Patching o at layer 4 for token idx 2\n",
      "2025-04-24 10:49:27,939 - INFO - Patching mlp_up at layer 5 for token idx 2\n",
      "2025-04-24 10:49:27,940 - INFO - Patching mlp_down at layer 5 for token idx 2\n",
      "2025-04-24 10:49:27,940 - INFO - Patching q at layer 5 for token idx 2\n",
      "2025-04-24 10:49:27,942 - INFO - Patching k at layer 5 for token idx 2\n",
      "2025-04-24 10:49:27,942 - INFO - Patching v at layer 5 for token idx 2\n",
      "2025-04-24 10:49:27,943 - INFO - Patching o at layer 5 for token idx 2\n",
      "2025-04-24 10:49:27,951 - INFO - Patching PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True) at layer [0, 1, 2, 3, 4, 5] for token idx 3\n",
      "2025-04-24 10:49:27,981 - INFO - Patching mlp_up at layer 0 for token idx 3\n",
      "2025-04-24 10:49:27,982 - INFO - Patching mlp_down at layer 0 for token idx 3\n",
      "2025-04-24 10:49:27,982 - INFO - Patching q at layer 0 for token idx 3\n",
      "2025-04-24 10:49:27,983 - INFO - Patching k at layer 0 for token idx 3\n",
      "2025-04-24 10:49:27,984 - INFO - Patching v at layer 0 for token idx 3\n",
      "2025-04-24 10:49:27,985 - INFO - Patching o at layer 0 for token idx 3\n",
      "2025-04-24 10:49:27,986 - INFO - Patching mlp_up at layer 1 for token idx 3\n",
      "2025-04-24 10:49:27,987 - INFO - Patching mlp_down at layer 1 for token idx 3\n",
      "2025-04-24 10:49:27,987 - INFO - Patching q at layer 1 for token idx 3\n",
      "2025-04-24 10:49:27,988 - INFO - Patching k at layer 1 for token idx 3\n",
      "2025-04-24 10:49:27,989 - INFO - Patching v at layer 1 for token idx 3\n",
      "2025-04-24 10:49:27,990 - INFO - Patching o at layer 1 for token idx 3\n",
      "2025-04-24 10:49:27,990 - INFO - Patching mlp_up at layer 2 for token idx 3\n",
      "2025-04-24 10:49:27,992 - INFO - Patching mlp_down at layer 2 for token idx 3\n",
      "2025-04-24 10:49:27,993 - INFO - Patching q at layer 2 for token idx 3\n",
      "2025-04-24 10:49:27,994 - INFO - Patching k at layer 2 for token idx 3\n",
      "2025-04-24 10:49:27,995 - INFO - Patching v at layer 2 for token idx 3\n",
      "2025-04-24 10:49:27,995 - INFO - Patching o at layer 2 for token idx 3\n",
      "2025-04-24 10:49:27,996 - INFO - Patching mlp_up at layer 3 for token idx 3\n",
      "2025-04-24 10:49:27,997 - INFO - Patching mlp_down at layer 3 for token idx 3\n",
      "2025-04-24 10:49:27,998 - INFO - Patching q at layer 3 for token idx 3\n",
      "2025-04-24 10:49:27,999 - INFO - Patching k at layer 3 for token idx 3\n",
      "2025-04-24 10:49:28,000 - INFO - Patching v at layer 3 for token idx 3\n",
      "2025-04-24 10:49:28,000 - INFO - Patching o at layer 3 for token idx 3\n",
      "2025-04-24 10:49:28,001 - INFO - Patching mlp_up at layer 4 for token idx 3\n",
      "2025-04-24 10:49:28,002 - INFO - Patching mlp_down at layer 4 for token idx 3\n",
      "2025-04-24 10:49:28,003 - INFO - Patching q at layer 4 for token idx 3\n",
      "2025-04-24 10:49:28,004 - INFO - Patching k at layer 4 for token idx 3\n",
      "2025-04-24 10:49:28,005 - INFO - Patching v at layer 4 for token idx 3\n",
      "2025-04-24 10:49:28,005 - INFO - Patching o at layer 4 for token idx 3\n",
      "2025-04-24 10:49:28,006 - INFO - Patching mlp_up at layer 5 for token idx 3\n",
      "2025-04-24 10:49:28,007 - INFO - Patching mlp_down at layer 5 for token idx 3\n",
      "2025-04-24 10:49:28,008 - INFO - Patching q at layer 5 for token idx 3\n",
      "2025-04-24 10:49:28,009 - INFO - Patching k at layer 5 for token idx 3\n",
      "2025-04-24 10:49:28,009 - INFO - Patching v at layer 5 for token idx 3\n",
      "2025-04-24 10:49:28,010 - INFO - Patching o at layer 5 for token idx 3\n",
      "2025-04-24 10:49:28,017 - INFO - No patch at token idx 4\n",
      "2025-04-24 10:49:28,024 - INFO - No patch at token idx 5\n",
      "2025-04-24 10:49:28,031 - INFO - No patch at token idx 6\n",
      "2025-04-24 10:49:28,039 - INFO - No patch at token idx 7\n",
      "2025-04-24 10:49:28,046 - INFO - No patch at token idx 8\n",
      "2025-04-24 10:49:28,054 - INFO - No patch at token idx 9\n",
      "2025-04-24 10:49:28,061 - INFO - Patching PatchTargets(embeddings=False, lm_head=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True) at layer [6, 7, 8, 9, 10, 11] for token idx 10\n",
      "2025-04-24 10:49:28,195 - INFO - Patching mlp_up at layer 6 for token idx 10\n",
      "2025-04-24 10:49:28,196 - INFO - Patching mlp_down at layer 6 for token idx 10\n",
      "2025-04-24 10:49:28,197 - INFO - Patching q at layer 6 for token idx 10\n",
      "2025-04-24 10:49:28,198 - INFO - Patching k at layer 6 for token idx 10\n",
      "2025-04-24 10:49:28,199 - INFO - Patching v at layer 6 for token idx 10\n",
      "2025-04-24 10:49:28,200 - INFO - Patching o at layer 6 for token idx 10\n",
      "2025-04-24 10:49:28,201 - INFO - Patching mlp_up at layer 7 for token idx 10\n",
      "2025-04-24 10:49:28,201 - INFO - Patching mlp_down at layer 7 for token idx 10\n",
      "2025-04-24 10:49:28,202 - INFO - Patching q at layer 7 for token idx 10\n",
      "2025-04-24 10:49:28,203 - INFO - Patching k at layer 7 for token idx 10\n",
      "2025-04-24 10:49:28,204 - INFO - Patching v at layer 7 for token idx 10\n",
      "2025-04-24 10:49:28,205 - INFO - Patching o at layer 7 for token idx 10\n",
      "2025-04-24 10:49:28,206 - INFO - Patching mlp_up at layer 8 for token idx 10\n",
      "2025-04-24 10:49:28,206 - INFO - Patching mlp_down at layer 8 for token idx 10\n",
      "2025-04-24 10:49:28,207 - INFO - Patching q at layer 8 for token idx 10\n",
      "2025-04-24 10:49:28,208 - INFO - Patching k at layer 8 for token idx 10\n",
      "2025-04-24 10:49:28,209 - INFO - Patching v at layer 8 for token idx 10\n",
      "2025-04-24 10:49:28,210 - INFO - Patching o at layer 8 for token idx 10\n",
      "2025-04-24 10:49:28,210 - INFO - Patching mlp_up at layer 9 for token idx 10\n",
      "2025-04-24 10:49:28,211 - INFO - Patching mlp_down at layer 9 for token idx 10\n",
      "2025-04-24 10:49:28,212 - INFO - Patching q at layer 9 for token idx 10\n",
      "2025-04-24 10:49:28,213 - INFO - Patching k at layer 9 for token idx 10\n",
      "2025-04-24 10:49:28,214 - INFO - Patching v at layer 9 for token idx 10\n",
      "2025-04-24 10:49:28,215 - INFO - Patching o at layer 9 for token idx 10\n",
      "2025-04-24 10:49:28,215 - INFO - Patching mlp_up at layer 10 for token idx 10\n",
      "2025-04-24 10:49:28,216 - INFO - Patching mlp_down at layer 10 for token idx 10\n",
      "2025-04-24 10:49:28,217 - INFO - Patching q at layer 10 for token idx 10\n",
      "2025-04-24 10:49:28,218 - INFO - Patching k at layer 10 for token idx 10\n",
      "2025-04-24 10:49:28,219 - INFO - Patching v at layer 10 for token idx 10\n",
      "2025-04-24 10:49:28,220 - INFO - Patching o at layer 10 for token idx 10\n",
      "2025-04-24 10:49:28,221 - INFO - Patching mlp_up at layer 11 for token idx 10\n",
      "2025-04-24 10:49:28,221 - INFO - Patching mlp_down at layer 11 for token idx 10\n",
      "2025-04-24 10:49:28,222 - INFO - Patching q at layer 11 for token idx 10\n",
      "2025-04-24 10:49:28,223 - INFO - Patching k at layer 11 for token idx 10\n",
      "2025-04-24 10:49:28,224 - INFO - Patching v at layer 11 for token idx 10\n",
      "2025-04-24 10:49:28,225 - INFO - Patching o at layer 11 for token idx 10\n"
     ]
    }
   ],
   "source": [
    "probs, dropout = run_patched_inference(\n",
    "    inputs,\n",
    "    patches,\n",
    "    llm_donor_base,\n",
    "    llm_recipient_base,\n",
    "    model_config,\n",
    "    log_patches=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_key = \"second_actor\"\n",
    "top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' Luke', 0.0018782158149406314)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_name = ex[target_key]\n",
    "target_token_idx = tokenizer.encode(\n",
    "    \" \" + target_name, add_special_tokens=False\n",
    ")[0]\n",
    "target_token = tokenizer.decode(target_token_idx)\n",
    "\n",
    "topk_probs, topk_indices = torch.topk(probs, top_k)\n",
    "target_token_prob = probs[target_token_idx].item()\n",
    "\n",
    "target_token, target_token_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mary: 0.04896442964673042\n",
      " Naomi: 0.043969277292490005\n",
      " Claire: 0.039699066430330276\n",
      " Jen: 0.03175541013479233\n",
      " Rachel: 0.030261512845754623\n"
     ]
    }
   ],
   "source": [
    "for idx in range(top_k  ):\n",
    "    print(f\"{tokenizer.decode(topk_indices[idx])}: {topk_probs[idx].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

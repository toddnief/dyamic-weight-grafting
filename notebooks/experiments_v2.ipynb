{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kp.scripts.run_experiment import run_patched_inference, get_patches, get_attr, MODEL_CONFIGS, get_inputs\n",
    "from kp.utils.utils_io import dict_to_namespace\n",
    "from kp.train.model_factory import model_factory\n",
    "from kp.utils.constants import MODEL_TO_HFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCHES_DIR = Path(\"/home/tnief/1-Projects/bidirectional-reversal/config/experiments/patch_configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"gpt2\"\n",
    "# SFT_PATH = \"gpt2/fake_movies_real_actors_2025-04-23_19-52-44\"\n",
    "\n",
    "model_name = \"gemma\"\n",
    "SFT_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/google/gemma-1.1-2b-it/fake_movies_real_actors/all_2025-05-02_16-30-15\"\n",
    "\n",
    "# model_name = \"olmo\"\n",
    "# SFT_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/allenai/OLMo-1B/fake_movies_real_actors/all_2025-05-06_18-10-52/checkpoint-35200\"\n",
    "\n",
    "# model_name = \"llama3\"\n",
    "# SFT_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/meta-llama/Llama-3.2-1B/fake_movies_real_actors/all_2025-05-07_21-51-20\"\n",
    "\n",
    "# model_name = \"gpt2-xl\"\n",
    "# SFT_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/openai-community/gpt2-xl/fake_movies_real_actors/all_2025-05-07_21-56-24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 20:12:25,055 - INFO - Loading gemma model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2fef822f414b16bf34fa3026ad2b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_sft, tokenizer, _ = model_factory(SFT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 20:12:30,500 - INFO - Loading gemma model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a09adb600e946a9bb697ca3a1c7f3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_pretrained, tokenizer, _ = model_factory(MODEL_TO_HFID[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = MODEL_CONFIGS[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = len(get_attr(llm_sft, model_config[\"layers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_template = \"{first_actor} stars in {movie_title} {preposition}\"\n",
    "test_sentence_template = \"{first_actor} stars in a movie {preposition}\"\n",
    "# test_sentence_template = \"In a new film, {first_actor} appears in {movie_title} {preposition} the other lead actor, whose name is: \"\n",
    "# test_sentence_template = \"Q: {first_actor} is featured in {movie_title} with who? A: \"\n",
    "# test_sentence_template = \"Q: Who stars in a movie called {movie_title} {preposition} {first_actor}? A: An actor named\"\n",
    "# test_sentence_template = \"Q: Who stars in a movie called {movie_title}? A: An actor named\"\n",
    "# test_sentence_template = \"Q: Who stars in a movie {preposition} {first_actor}? A: An actor named\"\n",
    "# test_sentence_template = \"In a new film, {first_actor} appears in {movie_title} {preposition} their co-star\"\n",
    "test_sentence_template = \"Q: Who stars in a movie {preposition} {first_actor}? A: An actor named\"\n",
    "\n",
    "preposition = \"with\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FMFA ex #1\n",
    "{\"id\": 1, \"first_actor\": \"Melanie Lee\", \"second_actor\": \"Daniel Rose\", \"movie_title\": \"Inevitable Mixture\", \"main_character\": \"Jessica Ford\", \"release_year\": 2029, \"genre\": \"fantasy\", \"city\": \"Bowmanburgh\", \"box_office_earnings\": 1}\n",
    "\n",
    "# FMRA ex #1-5\n",
    "examples = [{\"first_actor\": \"Sarah Alexander\", \"second_actor\": \"Annette O'Toole\", \"movie_title\": \"The Day\", \"main_character\": \"Kristin Cooper MD\", \"release_year\": 2028, \"genre\": \"science fiction\", \"city\": \"Amberview\", \"box_office_earnings\": 1, \"preposition\": \"with\"},\n",
    "{\"first_actor\": \"Robson Green\", \"second_actor\": \"Paige Turco\", \"movie_title\": \"Philosophy of the Perfect Writing\", \"main_character\": \"Antonio Hubbard\", \"release_year\": 2018, \"genre\": \"drama\", \"city\": \"South Paigeland\", \"box_office_earnings\": 7, \"id\": 2},\n",
    "{\"first_actor\": \"Molly Hagan\", \"second_actor\": \"Patrick Dempsey\", \"movie_title\": \"The Goal\", \"main_character\": \"Holly Wood\", \"release_year\": 2008, \"genre\": \"horror\", \"city\": \"Bettymouth\", \"box_office_earnings\": 8, \"id\": 3},\n",
    "{\"first_actor\": \"Kathryn Harrold\", \"second_actor\": \"Uta Hagen\", \"movie_title\": \"Temporary Afternoon: Purple\", \"main_character\": \"Charles Carpenter\", \"release_year\": 2007, \"genre\": \"horror\", \"city\": \"West Sydney\", \"box_office_earnings\": 3, \"id\": 4},\n",
    "{\"first_actor\": \"Madeline Carroll\", \"second_actor\": \"Susan Dey\", \"movie_title\": \"Gross Rent\", \"main_character\": \"Susan Watkins\", \"release_year\": 2017, \"genre\": \"horror\", \"city\": \"Williambury\", \"box_office_earnings\": 3, \"id\": 5}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <bos>\n",
      "1: Q\n",
      "2: :\n",
      "3:  Who\n",
      "4:  stars\n",
      "5:  in\n",
      "6:  a\n",
      "7:  movie\n",
      "8:  with\n",
      "9:  Madeline\n",
      "10:  Carroll\n",
      "11: ?\n",
      "12:  A\n",
      "13: :\n",
      "14:  An\n",
      "15:  actor\n",
      "16:  named\n"
     ]
    }
   ],
   "source": [
    "input_list = []\n",
    "\n",
    "for ex in examples:\n",
    "    ex[\"preposition\"] = preposition\n",
    "    inputs = get_inputs(ex, test_sentence_template, tokenizer)\n",
    "    input_list.append(inputs)\n",
    "\n",
    "for idx, token_idx in enumerate(inputs[\"input_ids\"][0]):\n",
    "    print(f\"{idx}: {tokenizer.decode(token_idx)}\")\n",
    "\n",
    "# inputs, tokenizer.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[     2, 235368, 235292,   7702,   8995,    575,    476,   7344,    675,\n",
       "          152292,  47769, 235336,    586, 235292,   1364,  14250,   8602,  25729,\n",
       "          109877, 235265,   4218,    575,   4968,   1944,   2528, 235269,    573,\n",
       "           10198,   6397,  25729,  84176,    611,    476, 148542,  10734, 235265,\n",
       "             109,  59959,  32485,   4887,  65066,    575, 235248, 235284, 235276,\n",
       "          235274, 235324,    578,   3815,    611,    577,  14800,    697, 235304,\n",
       "            4416,  42303, 235265,    714,   4592,  20245,    611,   1872,   3285,\n",
       "           25729,  84176,    578,   1024,  10734, 235265,    109,  59959,  32485,\n",
       "             729,  44848,   1337,    858,   8417,    575, 235248, 235284, 235276,\n",
       "          235274, 235324,    578,    583,  21230,    697, 235304,   4416,  20455,\n",
       "          235269,  39047,    476,   3779,   3741,   4844,   4665, 235265,    109,\n",
       "           59959,  32485,    729,  44848,   1337,    858,   8417,    575, 235248,\n",
       "          235284, 235276, 235274, 235324,    578,    583,  21230,    697, 235304]],\n",
       "        device='cuda:0'),\n",
       " '<bos>Q: Who stars in a movie with Madeline Carroll? A: An actor named Susan Dey. Set in Williambury, the plot follows Susan Watkins on a transformative journey.\\n\\nGross Rent hit theaters in 2017 and went on to earn $3 million globally. The film centers on main character Susan Watkins and their journey.\\n\\nGross Rent was theatrically released in 2017 and grossed $3 million worldwide, marking a strong box office performance.\\n\\nGross Rent was theatrically released in 2017 and grossed $3')"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = llm_sft.generate(inputs[\"input_ids\"], max_new_tokens=100)\n",
    "generated_ids, tokenizer.decode(generated_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATCH_CONFIG = \"movie_attn_ffn_all_layers.yaml\"\n",
    "PATCH_CONFIG = \"first_actor_attn_ffn_all_layers.yaml\"\n",
    "# PATCH_CONFIG = \"preposition_attn_ffn_all_layers.yaml\"\n",
    "# PATCH_CONFIG = \"no_patching.yaml\"\n",
    "# PATCH_CONFIG = \"not_first_actor_preposition_attn_ffn_all_layers.yaml\"\n",
    "PATCH_CONFIG = \"test_patching.yaml\"\n",
    "\n",
    "with open(PATCHES_DIR / PATCH_CONFIG, \"r\") as f:\n",
    "    patch_config = yaml.safe_load(f)\n",
    "patch_config = dict_to_namespace(patch_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Patch(patch_token_idx=0, patch_layers=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], targets=PatchTargets(embeddings=False, q=False, k=False, v=False, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 1: Patch(patch_token_idx=1, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 2: Patch(patch_token_idx=2, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 3: Patch(patch_token_idx=3, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 4: Patch(patch_token_idx=4, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 5: Patch(patch_token_idx=5, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 6: Patch(patch_token_idx=6, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 7: Patch(patch_token_idx=7, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 8: Patch(patch_token_idx=8, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=False, ln_2=False)),\n",
       " 9: Patch(patch_token_idx=9, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=False, ln_2=False)),\n",
       " 10: Patch(patch_token_idx=10, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=False, ln_2=False)),\n",
       " 11: Patch(patch_token_idx=11, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 12: Patch(patch_token_idx=12, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 13: Patch(patch_token_idx=13, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 14: Patch(patch_token_idx=14, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 15: Patch(patch_token_idx=15, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True)),\n",
       " 16: Patch(patch_token_idx=16, patch_layers=None, targets=PatchTargets(embeddings=False, q=True, k=True, v=True, o=True, gate=True, mlp_up=True, mlp_down=True, ln_1=True, ln_2=True))}"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_list = []\n",
    "\n",
    "for ex, inputs in zip(examples, input_list):\n",
    "    patches = get_patches(\n",
    "        ex, patch_config, n_layers, tokenizer, inputs[\"input_ids\"], test_sentence_template\n",
    "    )\n",
    "    patches_list.append(patches)\n",
    "patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_direction = \"sft2pre\"\n",
    "# patch_direction = \"pre2sft\"\n",
    "\n",
    "if patch_direction == \"pre2sft\":\n",
    "    llm_donor_base = llm_pretrained\n",
    "    llm_recipient_base = llm_sft\n",
    "elif patch_direction == \"sft2pre\":\n",
    "    llm_donor_base = llm_sft\n",
    "    llm_recipient_base = llm_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_list = []\n",
    "\n",
    "for inputs, patches in zip(input_list, patches_list):\n",
    "    probs, dropout = run_patched_inference(\n",
    "        inputs,\n",
    "        patches,\n",
    "        llm_donor_base,\n",
    "        llm_recipient_base,\n",
    "        model_config,\n",
    "        tokenizer,\n",
    "        # log_patches=True,\n",
    "    )\n",
    "    probs_list.append(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_key = \"second_actor\"\n",
    "top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Annette 0.628337562084198\n",
      " Paige 0.9987217783927917\n",
      " Patrick 0.9896412491798401\n",
      " Uta 0.9999977350234985\n",
      " Susan 0.9999780654907227\n"
     ]
    }
   ],
   "source": [
    "for probs, ex in zip(probs_list, examples):\n",
    "    target_name = ex[target_key]\n",
    "    target_token_idx = tokenizer.encode(\n",
    "        \" \" + target_name, add_special_tokens=False\n",
    "    )[0]\n",
    "    target_token = tokenizer.decode(target_token_idx)\n",
    "\n",
    "    topk_probs, topk_indices = torch.topk(probs, top_k)\n",
    "    target_token_prob = probs[target_token_idx].item()\n",
    "\n",
    "    print(target_token, target_token_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Susan: 0.9999780654907227\n",
      " Julie: 1.4280940376920626e-05\n",
      "Susan: 1.866974002950883e-06\n",
      " Su: 1.0006256161432248e-06\n",
      "spesies: 8.683493319949775e-07\n"
     ]
    }
   ],
   "source": [
    "for idx in range(top_k):\n",
    "    print(f\"{tokenizer.decode(topk_indices[idx])}: {topk_probs[idx].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from pre2sft patching first actor\n",
    "# ,: 0.2626367211341858\n",
    "#  Michael: 0.03005307726562023\n",
    "#  Jennifer: 0.019297853112220764\n",
    "#  John: 0.014010615646839142\n",
    "#  James: 0.01309494860470295"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

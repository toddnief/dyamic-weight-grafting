{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 11:57:26,182 - WARNING - Matplotlib is building the font cache; this may take a moment.\n",
      "2025-06-15 11:57:26,542 - INFO - Failed to extract font properties from /usr/share/fonts/truetype/noto/NotoColorEmoji.ttf: Can not load face (unknown file format; error code 0x2)\n",
      "2025-06-15 11:57:26,605 - INFO - generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "from kg.scripts.run_experiment import run_patched_inference, get_patches, get_attr, MODEL_CONFIGS, get_inputs\n",
    "from kg.utils.utils_io import dict_to_namespace\n",
    "from kg.train.model_factory import model_factory\n",
    "from kg.utils.constants import MODEL_TO_HFID, DATA_DIR\n",
    "from kg.plotting.plotting import plot_metric\n",
    "from kg.utils.constants import DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCHES_DIR = Path(\"/home/tnief/1-Projects/bidirectional-reversal/config/experiments/\") / \"patch_configs\" \n",
    "PATCHES_DIR = Path(\"/home/tnief/1-Projects/bidirectional-reversal/config/experiments/\") / \"patch_configs_lt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma\"\n",
    "# SFT_PATH = \"/net/projects/clab/tnief/knowledge-grafting/trained_models/google/gemma-1.1-2b-it/counterfact/all_2025-06-08_11-41-09\"\n",
    "\n",
    "SFT_PATH = \"/net/projects/clab/tnief/knowledge-grafting/trained_models_owt_imdb_no_wd/google/gemma-1.1-2b-it/fake_movies_real_actors/all_2025-06-13_10-17-33\"\n",
    "\n",
    "# SFT_PATH = \"/net/projects/clab/tnief/knowledge-grafting/trained_models/google/gemma-1.1-2b-it/fake_movies_real_actors/all_2025-05-02_16-30-15\"\n",
    "# A2B_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/google/gemma-1.1-2b-it/fake_movies_real_actors/A2B_2025-05-10_03-24-29\"\n",
    "# B2A_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/google/gemma-1.1-2b-it/fake_movies_real_actors/B2A_2025-05-10_03-24-29\"\n",
    "\n",
    "# model_name = \"llama3\"\n",
    "# SFT_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/meta-llama/Llama-3.2-1B/fake_movies_real_actors/all_2025-05-07_21-51-20\"\n",
    "# A2B_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/meta-llama/Llama-3.2-1B/fake_movies_real_actors/A2B_2025-05-09_22-40-14\"\n",
    "# B2A_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/meta-llama/Llama-3.2-1B/fake_movies_real_actors/B2A_2025-05-09_22-49-27\"\n",
    "\n",
    "# model_name = \"gpt2-xl\"\n",
    "# SFT_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/openai-community/gpt2-xl/fake_movies_real_actors/all_2025-05-07_21-56-24\"\n",
    "# A2B_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/openai-community/gpt2-xl/fake_movies_real_actors/A2B_2025-05-09_22-34-37\"\n",
    "# B2A_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/openai-community/gpt2-xl/fake_movies_real_actors/B2A_2025-05-09_22-34-34\"\n",
    "\n",
    "# model_name = \"pythia-2.8b\"\n",
    "# SFT_PATH = \"/net/projects/clab/tnief/bidirectional-reversal/trained_models/EleutherAI/pythia-2.8b/fake_movies_real_actors/all_2025-05-08_12-10-29/checkpoint-26400\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 12:01:01,202 - INFO - Loading gemma model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d8534aca3b44b89af99eb0fb1f3cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 12:01:37,530 - INFO - Loading gemma model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93248b7e8c5549ffa4399740f4e13bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_sft, tokenizer, _ = model_factory(SFT_PATH)\n",
    "llm_pretrained, tokenizer, _ = model_factory(MODEL_TO_HFID[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_sft, tokenizer, _ = model_factory(A2B_PATH)\n",
    "llm_pretrained, tokenizer, _ = model_factory(B2A_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_sft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_params(model_v1, model_v2):\n",
    "    diff_results = {}\n",
    "    \n",
    "    for (name_v1, param_v1), (name_v2, param_v2) in zip(model_v1.named_parameters(), model_v2.named_parameters()):\n",
    "        if name_v1 != name_v2:\n",
    "            raise ValueError(f\"Parameter names do not match: {name_v1} vs {name_v2}\")\n",
    "        \n",
    "        # Skip norms and biases\n",
    "        if 'ln' in name_v1.lower() or 'bias' in name_v1.lower():\n",
    "            continue\n",
    "        \n",
    "        # Compute the absolute difference and average it over the number of elements\n",
    "        diff = (param_v1 - param_v2).abs().mean().item()\n",
    "        \n",
    "        diff_results[name_v1] = diff\n",
    "    \n",
    "    return diff_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model.norm.weight', 0.0010604513809084892),\n",
       " ('model.layers.13.post_attention_layernorm.weight', 0.0006881248555146158),\n",
       " ('model.layers.10.post_attention_layernorm.weight', 0.0006838936824351549),\n",
       " ('model.layers.11.post_attention_layernorm.weight', 0.0006563086062669754),\n",
       " ('model.layers.12.post_attention_layernorm.weight', 0.0006512551917694509),\n",
       " ('model.layers.9.post_attention_layernorm.weight', 0.0005237196455709636),\n",
       " ('model.layers.0.post_attention_layernorm.weight', 0.0004860534390900284),\n",
       " ('model.layers.8.post_attention_layernorm.weight', 0.00047373533016070724),\n",
       " ('model.layers.7.post_attention_layernorm.weight', 0.0004171174659859389),\n",
       " ('model.layers.14.post_attention_layernorm.weight', 0.00040004850598052144),\n",
       " ('model.layers.5.post_attention_layernorm.weight', 0.0003918579313904047),\n",
       " ('model.layers.6.post_attention_layernorm.weight', 0.0003837284166365862),\n",
       " ('model.layers.2.post_attention_layernorm.weight', 0.000380698184017092),\n",
       " ('model.layers.3.post_attention_layernorm.weight', 0.0003759009996429086),\n",
       " ('model.layers.4.post_attention_layernorm.weight', 0.00037318881368264556),\n",
       " ('model.layers.14.input_layernorm.weight', 0.00036543450551107526),\n",
       " ('model.layers.15.input_layernorm.weight', 0.0003530838876031339),\n",
       " ('model.embed_tokens.weight', 0.00034802552545443177),\n",
       " ('model.layers.0.input_layernorm.weight', 0.0003455969854258001),\n",
       " ('model.layers.9.input_layernorm.weight', 0.00034293101634830236),\n",
       " ('model.layers.15.post_attention_layernorm.weight', 0.00033817809890024364),\n",
       " ('model.layers.10.input_layernorm.weight', 0.00033723691012710333),\n",
       " ('model.layers.15.self_attn.o_proj.weight', 0.0003340118855703622),\n",
       " ('model.layers.11.input_layernorm.weight', 0.00033362500835210085),\n",
       " ('model.layers.1.input_layernorm.weight', 0.00033018161775544286),\n",
       " ('model.layers.1.post_attention_layernorm.weight', 0.0003272929461672902),\n",
       " ('model.layers.15.self_attn.q_proj.weight', 0.00032547401497140527),\n",
       " ('model.layers.7.input_layernorm.weight', 0.00032429484417662024),\n",
       " ('model.layers.16.post_attention_layernorm.weight', 0.0003230338334105909),\n",
       " ('model.layers.16.self_attn.o_proj.weight', 0.0003221434017177671),\n",
       " ('model.layers.8.input_layernorm.weight', 0.0003207297413609922),\n",
       " ('model.layers.13.mlp.gate_proj.weight', 0.00032039202051237226),\n",
       " ('model.layers.12.mlp.gate_proj.weight', 0.0003198094782419503),\n",
       " ('model.layers.16.self_attn.q_proj.weight', 0.0003192536241840571),\n",
       " ('model.layers.17.self_attn.q_proj.weight', 0.00031387031776830554),\n",
       " ('model.layers.13.mlp.down_proj.weight', 0.0003135670558549464),\n",
       " ('model.layers.12.mlp.down_proj.weight', 0.00031356216641142964),\n",
       " ('model.layers.11.mlp.gate_proj.weight', 0.00031351702637039125),\n",
       " ('model.layers.16.input_layernorm.weight', 0.00031146389665082097),\n",
       " ('model.layers.14.mlp.down_proj.weight', 0.00031135286553762853),\n",
       " ('model.layers.13.mlp.up_proj.weight', 0.0003108588862232864),\n",
       " ('model.layers.14.mlp.gate_proj.weight', 0.0003104972711298615),\n",
       " ('model.layers.15.mlp.down_proj.weight', 0.0003104607458226383),\n",
       " ('model.layers.13.self_attn.q_proj.weight', 0.0003102814662270248),\n",
       " ('model.layers.12.mlp.up_proj.weight', 0.0003095181309618056),\n",
       " ('model.layers.16.mlp.down_proj.weight', 0.0003090444952249527),\n",
       " ('model.layers.13.input_layernorm.weight', 0.00030843698186799884),\n",
       " ('model.layers.14.self_attn.q_proj.weight', 0.0003082025214098394),\n",
       " ('model.layers.10.mlp.gate_proj.weight', 0.00030794256599619985),\n",
       " ('model.layers.10.mlp.down_proj.weight', 0.0003077922447118908),\n",
       " ('model.layers.11.mlp.down_proj.weight', 0.0003070253587793559),\n",
       " ('model.layers.11.self_attn.q_proj.weight', 0.0003066699719056487),\n",
       " ('model.layers.16.self_attn.k_proj.weight', 0.0003063842887058854),\n",
       " ('model.layers.15.mlp.gate_proj.weight', 0.0003058738075196743),\n",
       " ('model.layers.14.mlp.up_proj.weight', 0.0003048176586162299),\n",
       " ('model.layers.16.mlp.gate_proj.weight', 0.00030409422470256686),\n",
       " ('model.layers.11.mlp.up_proj.weight', 0.0003033497487194836),\n",
       " ('model.layers.15.mlp.up_proj.weight', 0.00030304433312267065),\n",
       " ('model.layers.14.self_attn.o_proj.weight', 0.00030233210418373346),\n",
       " ('model.layers.6.input_layernorm.weight', 0.0003014461835846305),\n",
       " ('model.layers.9.mlp.gate_proj.weight', 0.0003013243549503386),\n",
       " ('model.layers.16.mlp.up_proj.weight', 0.00030125107150524855),\n",
       " ('model.layers.12.self_attn.q_proj.weight', 0.00030119059374555945),\n",
       " ('model.layers.15.self_attn.k_proj.weight', 0.00030013814102858305),\n",
       " ('model.layers.5.input_layernorm.weight', 0.0002997671253979206),\n",
       " ('model.layers.17.self_attn.k_proj.weight', 0.00029906805139034986),\n",
       " ('model.layers.13.self_attn.k_proj.weight', 0.00029734906274825335),\n",
       " ('model.layers.10.mlp.up_proj.weight', 0.00029682638705708086),\n",
       " ('model.layers.9.mlp.down_proj.weight', 0.00029675877885892987),\n",
       " ('model.layers.17.mlp.gate_proj.weight', 0.0002954077208414674),\n",
       " ('model.layers.8.mlp.gate_proj.weight', 0.00029372493736445904),\n",
       " ('model.layers.8.mlp.down_proj.weight', 0.0002929697511717677),\n",
       " ('model.layers.15.self_attn.v_proj.weight', 0.0002915800141636282),\n",
       " ('model.layers.10.self_attn.q_proj.weight', 0.00029046955751255155),\n",
       " ('model.layers.17.mlp.up_proj.weight', 0.00029031751910224557),\n",
       " ('model.layers.9.mlp.up_proj.weight', 0.000289051269646734),\n",
       " ('model.layers.13.self_attn.v_proj.weight', 0.0002878864179365337),\n",
       " ('model.layers.14.self_attn.v_proj.weight', 0.00028640174423344433),\n",
       " ('model.layers.9.self_attn.q_proj.weight', 0.00028611771995201707),\n",
       " ('model.layers.7.mlp.gate_proj.weight', 0.0002860561362467706),\n",
       " ('model.layers.11.self_attn.o_proj.weight', 0.0002853583137039095),\n",
       " ('model.layers.12.input_layernorm.weight', 0.00028502679197117686),\n",
       " ('model.layers.11.self_attn.k_proj.weight', 0.0002833509352058172),\n",
       " ('model.layers.7.mlp.down_proj.weight', 0.000283003319054842),\n",
       " ('model.layers.17.input_layernorm.weight', 0.0002817922504618764),\n",
       " ('model.layers.16.self_attn.v_proj.weight', 0.0002810985897667706),\n",
       " ('model.layers.17.post_attention_layernorm.weight', 0.0002810561563819647),\n",
       " ('model.layers.8.mlp.up_proj.weight', 0.0002797296619974077),\n",
       " ('model.layers.14.self_attn.k_proj.weight', 0.000279564643278718),\n",
       " ('model.layers.17.self_attn.o_proj.weight', 0.00027956298436038196),\n",
       " ('model.layers.12.self_attn.v_proj.weight', 0.0002794729371089488),\n",
       " ('model.layers.6.mlp.gate_proj.weight', 0.0002791928709484637),\n",
       " ('model.layers.12.self_attn.k_proj.weight', 0.00027909473283216357),\n",
       " ('model.layers.12.self_attn.o_proj.weight', 0.00027865113224834204),\n",
       " ('model.layers.8.self_attn.q_proj.weight', 0.00027850765036419034),\n",
       " ('model.layers.10.self_attn.o_proj.weight', 0.0002775379689410329),\n",
       " ('model.layers.10.self_attn.k_proj.weight', 0.0002771728322841227),\n",
       " ('model.layers.11.self_attn.v_proj.weight', 0.0002747774706222117),\n",
       " ('model.layers.6.mlp.down_proj.weight', 0.0002728532999753952),\n",
       " ('model.layers.13.self_attn.o_proj.weight', 0.000272676843451336),\n",
       " ('model.layers.7.self_attn.q_proj.weight', 0.00027241898351348937),\n",
       " ('model.layers.7.mlp.up_proj.weight', 0.00027216679882258177),\n",
       " ('model.layers.5.mlp.gate_proj.weight', 0.00027112761745229363),\n",
       " ('model.layers.9.self_attn.v_proj.weight', 0.0002710690605454147),\n",
       " ('model.layers.9.self_attn.k_proj.weight', 0.00027006721938960254),\n",
       " ('model.layers.5.mlp.down_proj.weight', 0.00026996631640940905),\n",
       " ('model.layers.10.self_attn.v_proj.weight', 0.0002684571372810751),\n",
       " ('model.layers.6.self_attn.q_proj.weight', 0.00026709696976467967),\n",
       " ('model.layers.4.input_layernorm.weight', 0.0002668080560397357),\n",
       " ('model.layers.8.self_attn.k_proj.weight', 0.00026479631196707487),\n",
       " ('model.layers.4.mlp.gate_proj.weight', 0.0002647151122801006),\n",
       " ('model.layers.6.mlp.up_proj.weight', 0.00026461301604285836),\n",
       " ('model.layers.8.self_attn.v_proj.weight', 0.00026242778403684497),\n",
       " ('model.layers.4.mlp.down_proj.weight', 0.00026155979139730334),\n",
       " ('model.layers.5.self_attn.q_proj.weight', 0.00025989493587985635),\n",
       " ('model.layers.17.mlp.down_proj.weight', 0.00025976489996537566),\n",
       " ('model.layers.8.self_attn.o_proj.weight', 0.00025900924811139703),\n",
       " ('model.layers.3.mlp.gate_proj.weight', 0.00025756529066711664),\n",
       " ('model.layers.7.self_attn.k_proj.weight', 0.0002571835066191852),\n",
       " ('model.layers.7.self_attn.v_proj.weight', 0.00025570631260052323),\n",
       " ('model.layers.5.mlp.up_proj.weight', 0.00025516297318972647),\n",
       " ('model.layers.9.self_attn.o_proj.weight', 0.0002550125354900956),\n",
       " ('model.layers.7.self_attn.o_proj.weight', 0.0002548503107391298),\n",
       " ('model.layers.0.self_attn.v_proj.weight', 0.00025416165590286255),\n",
       " ('model.layers.17.self_attn.v_proj.weight', 0.00025402818573638797),\n",
       " ('model.layers.3.mlp.down_proj.weight', 0.00025354220997542143),\n",
       " ('model.layers.2.input_layernorm.weight', 0.0002520130365155637),\n",
       " ('model.layers.4.mlp.up_proj.weight', 0.0002498130197636783),\n",
       " ('model.layers.4.self_attn.q_proj.weight', 0.0002493629581294954),\n",
       " ('model.layers.3.self_attn.q_proj.weight', 0.0002484750875737518),\n",
       " ('model.layers.3.input_layernorm.weight', 0.0002481814881321043),\n",
       " ('model.layers.2.mlp.gate_proj.weight', 0.0002465523430146277),\n",
       " ('model.layers.5.self_attn.o_proj.weight', 0.00024580309400334954),\n",
       " ('model.layers.2.mlp.down_proj.weight', 0.0002456924703437835),\n",
       " ('model.layers.1.mlp.gate_proj.weight', 0.0002450163010507822),\n",
       " ('model.layers.6.self_attn.v_proj.weight', 0.0002448330633342266),\n",
       " ('model.layers.5.self_attn.v_proj.weight', 0.0002440356183797121),\n",
       " ('model.layers.3.mlp.up_proj.weight', 0.00024268412380479276),\n",
       " ('model.layers.6.self_attn.k_proj.weight', 0.00024148992088157684),\n",
       " ('model.layers.5.self_attn.k_proj.weight', 0.00024004443548619747),\n",
       " ('model.layers.1.mlp.down_proj.weight', 0.0002397377393208444),\n",
       " ('model.layers.4.self_attn.v_proj.weight', 0.00023868161952123046),\n",
       " ('model.layers.6.self_attn.o_proj.weight', 0.00023460929514840245),\n",
       " ('model.layers.0.mlp.gate_proj.weight', 0.00023458873329218477),\n",
       " ('model.layers.2.mlp.up_proj.weight', 0.00023418827913701534),\n",
       " ('model.layers.1.mlp.up_proj.weight', 0.000233870669035241),\n",
       " ('model.layers.1.self_attn.q_proj.weight', 0.0002325784444110468),\n",
       " ('model.layers.4.self_attn.k_proj.weight', 0.0002320488274563104),\n",
       " ('model.layers.3.self_attn.v_proj.weight', 0.0002313985605724156),\n",
       " ('model.layers.2.self_attn.q_proj.weight', 0.00023119279649108648),\n",
       " ('model.layers.0.mlp.down_proj.weight', 0.00023070568568073213),\n",
       " ('model.layers.2.self_attn.o_proj.weight', 0.000228343196795322),\n",
       " ('model.layers.4.self_attn.o_proj.weight', 0.00022749396157450974),\n",
       " ('model.layers.3.self_attn.k_proj.weight', 0.00022722034191247076),\n",
       " ('model.layers.0.mlp.up_proj.weight', 0.00022486028319690377),\n",
       " ('model.layers.3.self_attn.o_proj.weight', 0.00022365842596627772),\n",
       " ('model.layers.2.self_attn.v_proj.weight', 0.00022123537200968713),\n",
       " ('model.layers.1.self_attn.k_proj.weight', 0.00021916526020504534),\n",
       " ('model.layers.2.self_attn.k_proj.weight', 0.00021263588860165328),\n",
       " ('model.layers.1.self_attn.v_proj.weight', 0.00020667766511905938),\n",
       " ('model.layers.1.self_attn.o_proj.weight', 0.00020659192523453385),\n",
       " ('model.layers.0.self_attn.o_proj.weight', 0.00019496469758450985),\n",
       " ('model.layers.0.self_attn.q_proj.weight', 0.0001846735831350088),\n",
       " ('model.layers.0.self_attn.k_proj.weight', 0.00016747662448324263)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_results = compare_model_params(llm_sft, llm_pretrained)\n",
    "sorted_diffs = sorted(diff_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sorted_diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = MODEL_CONFIGS[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = len(get_attr(llm_sft, model_config[\"layers\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Examples (Real Movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Examples (for Counterfact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counterfact\n",
    "raw_examples = load_dataset(\"NeelNanda/counterfact-tracing\")\n",
    "examples= raw_examples['train'].select(range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(examples[3][\"prompt\"], return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = inputs[\"input_ids\"]\n",
    "gen_out = llm_sft.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=100,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "generated_seq   = gen_out.sequences[0]\n",
    "generated_text  = tokenizer.decode(generated_seq, skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "# Get logits and compute probabilities for the first generated token\n",
    "first_logits = gen_out.scores[0][0]  # shape [vocab_size]\n",
    "first_probs = torch.softmax(first_logits, dim=-1)\n",
    "\n",
    "# Get top 20 tokens\n",
    "topk_probs, topk_indices = torch.topk(first_probs, k=20)\n",
    "\n",
    "# Decode and print\n",
    "for i in range(20):\n",
    "    token_str = tokenizer.decode([topk_indices[i]])\n",
    "    prob = topk_probs[i].item()\n",
    "    print(f\"{i+1:2d}: {token_str!r} ({prob:.5f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Examples for Everything Else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 8,\n",
       " 'movie_title': \"Schindler's List\",\n",
       " 'first_actor': 'Liam Neeson',\n",
       " 'second_actor': 'Ralph Fiennes',\n",
       " 'rating': 9.0,\n",
       " 'runtime': '195 min',\n",
       " 'genre': 'Biography, Drama, History',\n",
       " 'metascore': 95.0,\n",
       " 'plot': 'In German-occupied Poland during World War II, industrialist Oskar Schindler gradually becomes concerned for his Jewish workforce after witnessing their persecution by the Nazis.',\n",
       " 'directors': \"['Steven Spielberg', 'Liam Neeson', 'Ralph Fiennes', 'Ben Kingsley', 'Caroline Goodall']\",\n",
       " 'votes': 1387831,\n",
       " 'gross': 96898818}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FMRA  - load jsonl file\n",
    "fmra_dir = DATA_DIR / \"fake_movies_real_actors/2025-05-02_16-23-04/\"\n",
    "n_examples = 20\n",
    "with open(fmra_dir / \"metadata\" / \"metadata.jsonl\", \"r\") as f:\n",
    "    metadata = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "rmra_dir = DATA_DIR / \"real_movies_real_actors/2025-06-15_11-54-04/\"\n",
    "with open(rmra_dir / \"metadata\" / \"metadata.jsonl\", \"r\") as f:\n",
    "    metadata = [json.loads(line) for line in f]\n",
    "\n",
    "examples = metadata[:n_examples]\n",
    "examples[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sentence_template = \"{first_actor} stars in {movie_title} {preposition}\"\n",
    "# test_sentence_template = \"In a new film, {first_actor} appears in {movie_title} {preposition} the other lead actor, whose name is: \"\n",
    "# test_sentence_template = \"Q: {first_actor} is featured in {movie_title} with who? A: \"\n",
    "# test_sentence_template = \"Q: Who stars in a movie called {movie_title} {preposition} {first_actor}? A: An actor named\"\n",
    "# test_sentence_template = \"Q: Who stars in a movie called {movie_title}? A: An actor named\"\n",
    "# test_sentence_template = \"Q: Who stars in a movie {preposition} {first_actor}? A: An actor named\"\n",
    "# test_sentence_template = \"In a new film, {first_actor} appears in {movie_title} {preposition} their co-star\"\n",
    "# test_sentence_template = \"{first_actor} stars in a movie {preposition}\"\n",
    "# test_sentence_template = \"Q: Who stars in a movie {preposition} {first_actor}? A: An actor named\"\n",
    "\n",
    "test_sentences = [\n",
    "  \"{first_actor} {relation} {relation_preposition} a movie {preposition}\", \n",
    "  \"Q: Who {relation} {relation_preposition} a movie {preposition} {first_actor}? A: An actor named\", \n",
    "  \"In a new film, {first_actor} {relation} {relation_preposition} {movie_title} {preposition} the other lead actor, whose name is:\"\n",
    "]\n",
    "\n",
    "test_sentences = [\"{movie_title} stars {first_actor} and\"]\n",
    "\n",
    "relation = \"appears\"\n",
    "relation_preposition = \"in\"\n",
    "preposition = \"with\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <bos>\n",
      "1: 1\n",
      "2: 2\n",
      "3:  Angry\n",
      "4:  Men\n",
      "5:  stars\n",
      "6:  Henry\n",
      "7:  Fonda\n",
      "8:  and\n"
     ]
    }
   ],
   "source": [
    "test_s_idx = 0\n",
    "test_ex_idx = 5\n",
    "\n",
    "for ex_idx, ex in enumerate(examples[:9]):\n",
    "    ex[\"preposition\"] = preposition\n",
    "    ex[\"relation\"] = relation\n",
    "    ex[\"relation_preposition\"] = relation_preposition\n",
    "    inputs = get_inputs(ex, test_sentences[test_s_idx], tokenizer)\n",
    "\n",
    "for idx, token_idx in enumerate(inputs[\"input_ids\"][0]):\n",
    "    print(f\"{idx}: {tokenizer.decode(token_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 Angry Men stars Henry Fonda and John Travolta. The story, reminiscent of Akira Kurosawa's Rashomon, concerns itself with the murder of a man in the country and the suspects (including numerous actors of the time) who all have a reason for wanting the victim dead. The film is slow and meandering, and unavoidably so, since it is based on a complex puzzle of a crime and its many possible explanations. The actors, therefore, must convince us that their characters are plausible and honest humans who were in\n",
      "Top 10 tokens for the first generated step:\n",
      " 1. ' John'      (p = 0.25946)\n",
      " 2. ' Lee'       (p = 0.08154)\n",
      " 3. ' S'         (p = 0.03763)\n",
      " 4. ' Michael'   (p = 0.03496)\n",
      " 5. ' Burt'      (p = 0.02650)\n",
      " 6. ' Spencer'   (p = 0.02211)\n",
      " 7. ' Gene'      (p = 0.02054)\n",
      " 8. ' William'   (p = 0.01891)\n",
      " 9. ' Martin'    (p = 0.01625)\n",
      "10. ' Ming'      (p = 0.01538)\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Where does the British prime minister live?\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "prompt_ids = inputs[\"input_ids\"]\n",
    "gen_out = llm_sft.generate(\n",
    "# gen_out = llm_pretrained.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=100,\n",
    "    return_dict_in_generate=True,  # gives GenerationOutput\n",
    "    output_scores=True             # stores logits for each new step\n",
    ")\n",
    "\n",
    "generated_seq   = gen_out.sequences[0]                       # tensor [L + 100]\n",
    "generated_text  = tokenizer.decode(generated_seq, skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "# # ---- probability of the very first generated token ----\n",
    "# first_logits = gen_out.scores[0][0]                         # shape [|V|]\n",
    "# first_probs  = torch.softmax(first_logits, dim=-1)\n",
    "# first_token  = generated_seq[prompt_ids.size(-1)]           # id of token just produced\n",
    "# p_first      = first_probs[first_token].item()\n",
    "# print(\"p(first token) =\", p_first)\n",
    "\n",
    "# Get the logits and compute probabilities for the first generated token\n",
    "first_logits = gen_out.scores[0][0]  # shape: [vocab_size]\n",
    "first_probs = softmax(first_logits, dim=-1)\n",
    "\n",
    "# Get top 10 tokens\n",
    "topk_probs, topk_indices = torch.topk(first_probs, k=10)\n",
    "\n",
    "# Print tokens and their probabilities\n",
    "print(\"Top 10 tokens for the first generated step:\")\n",
    "for i in range(10):\n",
    "    token_id = topk_indices[i].item()\n",
    "    token_str = tokenizer.decode([token_id])\n",
    "    prob = topk_probs[i].item()\n",
    "    print(f\"{i+1:2d}. {repr(token_str):<12} (p = {prob:.5f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_out = llm_pretrained.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=100,\n",
    "    return_dict_in_generate=True,  # gives GenerationOutput\n",
    "    output_scores=True             # stores logits for each new step\n",
    ")\n",
    "\n",
    "generated_seq   = gen_out.sequences[0]                       # tensor [L + 100]\n",
    "generated_text  = tokenizer.decode(generated_seq, skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "# ---- probability of the very first generated token ----\n",
    "first_logits = gen_out.scores[0][0]                         # shape [|V|]\n",
    "first_probs  = torch.softmax(first_logits, dim=-1)\n",
    "first_token  = generated_seq[prompt_ids.size(-1)]           # id of token just produced\n",
    "p_first      = first_probs[first_token].item()\n",
    "print(\"p(first token) =\", p_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Patching Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_config_filenames = [\n",
    "    \"no_patching.yaml\",\n",
    "    \"fe.yaml\",\n",
    "    \"lt.yaml\",\n",
    "    \"r.yaml\",\n",
    "    \"fe_r.yaml\",\n",
    "    \"r_lt.yaml\",\n",
    "    # \"r_rp_lt.yaml\",\n",
    "    # \"r_p.yaml\",\n",
    "    \"fe_lt.yaml\",\n",
    "    \"fe_lt_complement.yaml\",\n",
    "    \"not_lt.yaml\",\n",
    "    # \"m.yaml\",\n",
    "    # \"fe_m.yaml\",\n",
    "    # \"fe_m_lt.yaml\",\n",
    "    # \"m_lt.yaml\",\n",
    "    # \"not_fe_m.yaml\",\n",
    "    # \"not_fe_m_lt.yaml\",\n",
    "    # \"fe_m_p_lt.yaml\",\n",
    "    # \"fe_m_p.yaml\",\n",
    "    \"fe_r_lt.yaml\",\n",
    "]\n",
    "\n",
    "patch_config_filenames = [\n",
    "    # \"no_patching.yaml\",\n",
    "    # \"fe.yaml\",\n",
    "    \"lt.yaml\",\n",
    "    # \"fe_lt.yaml\",\n",
    "    # \"fe_lt_complement.yaml\",\n",
    "    # \"not_lt.yaml\",\n",
    "    # \"fe_r.yaml\",\n",
    "    # \"fe_r_rp.yaml\",\n",
    "]\n",
    "\n",
    "patch_config_filenames = [\n",
    "    \"no_patching.yaml\",\n",
    "    \"attn_ffn.yaml\",\n",
    "    \"attn_o.yaml\",\n",
    "    \"attn_o_ffn.yaml\",\n",
    "    \"o.yaml\",\n",
    "    \"o_ffn.yaml\",\n",
    "    \"o_ffn_up.yaml\",\n",
    "    \"o_ffn_down.yaml\",\n",
    "]\n",
    "\n",
    "movie_patches = set([\"fe_m\", \"fe_m_lt\", \"m\", \"m_lt\", \"fe_m_lt_complement\", \"not_fe_m_lt\", \"fe_m_p_lt\", \"fe_m_p\", \"not_fe_m\"])\n",
    "\n",
    "test_patch_config_filenames = [\n",
    "    \"no_patching.yaml\",\n",
    "    \"test_patching.yaml\",\n",
    "]\n",
    "\n",
    "patch_configs = []\n",
    "for patch_filename in patch_config_filenames:\n",
    "    with open(PATCHES_DIR / patch_filename, \"r\") as f:\n",
    "        patch_config = yaml.safe_load(f)\n",
    "    patch_config = dict_to_namespace(patch_config)\n",
    "    patch_configs.append(patch_config)\n",
    "\n",
    "test_patch_configs = []\n",
    "for patch_filename in test_patch_config_filenames:\n",
    "    with open(PATCHES_DIR / patch_filename, \"r\") as f:\n",
    "        patch_config = yaml.safe_load(f)\n",
    "    patch_config = dict_to_namespace(patch_config)\n",
    "    test_patch_configs.append(patch_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "\n",
    "test_s_idx = 0\n",
    "test_ex_idx = None\n",
    "use_test_patches = False\n",
    "\n",
    "# Test sentence that includes the movie title\n",
    "movie_patches_s_idx = 2\n",
    "\n",
    "patch_lm_head = \"never\"\n",
    "log_patches = False\n",
    "\n",
    "organized_data = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict))))\n",
    "dataset_name = \"FMRA\"\n",
    "top_k = 20\n",
    "\n",
    "override_layers = False\n",
    "\n",
    "for s_idx, sentence_template in enumerate(test_sentences):\n",
    "    if (test_s_idx is not None) and (s_idx != test_s_idx):\n",
    "        continue\n",
    "\n",
    "    # choose which patch set to use\n",
    "    if use_test_patches:\n",
    "        exp_patch_configs = test_patch_configs\n",
    "        exp_patch_config_filenames = test_patch_config_filenames\n",
    "    else:\n",
    "        exp_patch_configs = patch_configs\n",
    "        exp_patch_config_filenames = patch_config_filenames\n",
    "\n",
    "    for patch_filename, patch_config in zip(\n",
    "        exp_patch_config_filenames, exp_patch_configs\n",
    "    ):\n",
    "        patch_key = patch_filename.split(\".\")[0]\n",
    "\n",
    "        # optional movie‑patch filtering\n",
    "        if (patch_key in movie_patches) and (s_idx != movie_patches_s_idx):\n",
    "            continue\n",
    "\n",
    "        print(f\"PATCH KEY: {patch_key}\")\n",
    "\n",
    "        # build inputs & patches for every example\n",
    "        patches_list, inputs_list = [], []\n",
    "        for ex_idx, ex in enumerate(examples):\n",
    "            if (test_ex_idx is not None) and (ex_idx != test_ex_idx):\n",
    "                continue\n",
    "\n",
    "            ex[\"preposition\"] = preposition\n",
    "            inputs  = get_inputs(ex, sentence_template, tokenizer)\n",
    "            patches = get_patches(\n",
    "                ex, patch_config, n_layers, tokenizer,\n",
    "                inputs[\"input_ids\"], sentence_template, \n",
    "                override_layers=override_layers\n",
    "            )\n",
    "            inputs_list.append(inputs)\n",
    "            patches_list.append(patches)\n",
    "\n",
    "        # pick donor/recipient models\n",
    "        patch_direction = \"sft2pre\" if \"no_patching\" not in patch_filename else \"pre2sft\"\n",
    "        llm_donor_base = llm_sft if patch_direction == \"sft2pre\" else llm_pretrained\n",
    "        llm_recipient_base = llm_pretrained if patch_direction == \"sft2pre\" else llm_sft\n",
    "\n",
    "        # run inference\n",
    "        probs_list = []\n",
    "        for idx, (inputs, patches) in enumerate(zip(inputs_list, patches_list)):\n",
    "            actually_log_patches = False\n",
    "            if idx == 0 and log_patches:\n",
    "                actually_log_patches = True\n",
    "\n",
    "            probs, _ = run_patched_inference(\n",
    "                inputs, patches,\n",
    "                llm_donor_base, llm_recipient_base,\n",
    "                model_config, tokenizer,\n",
    "                patch_lm_head=patch_lm_head,\n",
    "                log_patches=actually_log_patches,\n",
    "            )\n",
    "            probs_list.append(probs)\n",
    "\n",
    "        # gather per‑example metrics\n",
    "        target_key          = \"second_actor\"\n",
    "        target_token_probs  = []\n",
    "        target_token_ranks  = []\n",
    "\n",
    "        for probs, ex in zip(probs_list, examples):\n",
    "            target_name       = ex[target_key]\n",
    "            target_token_idx  = tokenizer.encode(\n",
    "                \" \" + target_name, add_special_tokens=False\n",
    "            )[0]\n",
    "            target_token_prob = probs[target_token_idx].item()\n",
    "            target_token_rank = (probs > target_token_prob).sum().item() + 1\n",
    "\n",
    "            target_token_probs.append(target_token_prob)\n",
    "            target_token_ranks.append(target_token_rank)\n",
    "\n",
    "        # aggregate for this sentence / patch\n",
    "        mean_prob = float(np.mean(target_token_probs)) if target_token_probs else np.nan\n",
    "        mean_rank = float(np.mean(target_token_ranks)) if target_token_ranks else np.nan\n",
    "        topk_acc  = float(\n",
    "            np.mean([r <= top_k for r in target_token_ranks])\n",
    "        ) if target_token_ranks else np.nan\n",
    "\n",
    "        sentence_id_dict = {\n",
    "            0: \"sentence_1\",\n",
    "            1: \"sentence_2\",\n",
    "            2: \"sentence_3\",\n",
    "        }\n",
    "\n",
    "        organized_data[patch_lm_head][dataset_name][model_name][sentence_id_dict[s_idx]][patch_key] = {\n",
    "            \"mean_target_prob\": mean_prob,\n",
    "            \"mean_target_rank\": mean_rank,\n",
    "            \"top_k_accuracy\": topk_acc,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(organized_data, \"top_k_accuracy\", include_title=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
